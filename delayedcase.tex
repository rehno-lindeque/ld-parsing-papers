
\documentclass[a4paper,11pt]{article}

%%%%%%%%%%%%%%%%%%% PACKAGES %%%%%%%%%%%%%%%%%%%
\usepackage{tikz}
\usepackage{geometry}
\geometry{a4paper}
\usepackage{gcl}
%\usepackage{pfs}
%\usepackage{algorithmic}
%\usepackage{graphicx}
%\usepackage{epstopdf}
%\usepackage{natbib}
%\usepackage{fancyvrb}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{proof}

%\usepackage{stmaryrd}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}

%%%%%%%%%%%%%%%%%%%% MACROS %%%%%%%%%%%%%%%%%%%%
% Grammars
%\newcommand {\GRAMMAR}
%  {\begin{indent} \begin{align*}}
%  %{\setlength{\parindent}{\GRAMMAR}\\
%  %\begin{indent}}

%\newcommand {\ENDGRAMMAR}
%  {\end{align*} \end{indent}}

\newenvironment{grammar}{\begin{align*}}{\end{align*}}
  
%  {\begin{indent} \begin{align*}}
%  {\end{align*} \end{indent}}
  
%\setlength{\topsep}{2mm}<
%\setlength{\parindent}{15pt}

%\newlength{\OBLS}\setlength{\OBLS}{\baselineskip}
%\newlength{\OPAR}\setlength{\OPAR}{\parindent}
%\newcommand{\pgskip}{\hspace{\mathindent}\=\+}
%\newcommand{\FOR}[1]{\FRM{\,#1\,}}
%\newcommand{\FRM}[1]{\mbox{$#1$}}
%\newcommand{\DERIVE}
%  {\setlength{\baselineskip}{1.3\baselineskip}
%  \begin{tabbing}
%  \hspace{\OPAR}\=\hspace{\OPAR}\=\hspace{\OPAR}\=\hspace{\OPAR}\=\hspace{\OPAR}\=\hspace{\OPAR}\=\kill
%  }
%\newcommand{\form}[1]
%  {\>\>\FOR{#1}}
%\newcommand{\hint}[2]
%  {\\*\>\FRM{#1}\>\>\{#2\}\\}
%\newcommand{\STARTPROOF}[1]
%  {\TOPROVE{#1}\\$\vartriangleright$ \>\>}
%\newcommand{\TOPROVE}[1]
%  {#1}
%\newcommand{\ENDDERIVE}
%  {\end{tabbing}\setlength{\baselineskip}{\OBLS}
%}
%\newcommand{\ENDPROOF}
%  {$\rbrack\arrowvert$\\
%  $\square$\\\\}
%\newcommand {\IND}
%  {\setlength{\parindent}{\OPAR}\\
%  \begin{indent}}
%\newcommand {\ENDIND}
%  {\end{indent}\\
%  \setlength{\parindent}{0 pt}\\}
%\newcommand {\IF}
%  {\mathbf{if} \:\:}
%\newcommand {\BAR}
%  {\ [\!] \:\:\:}
%\newcommand {\FI}
%  {\mathbf{fi} \:\:}
%\newcommand {\ARROW}
%  {\rightarrow}
%\newcommand{\clearemptydoublepage}{\newpage{\pagestyle{empty}\cleardoublepage}}
%\newcommand{\TT} {\textit{true}}
%\newcommand{\FF} {\textit{false}}


%%%%%%%%%%%%%%%%%%%% MACROS %%%%%%%%%%%%%%%%%%%%
\floatstyle{plain}
\newfloat{grammar}{thp}{lop}
\floatname{grammar}{Grammar}


%%%%%%%%%%%%%%%%%%%%%Stuff from SICL article%%%%%%
%\newcommand{\sos}[1]{\mathcal{#1}}
%\newcommand{\sset}[1]{\texttt{#1}}
%\newcommand{\restr}[2]{#1\restriction#2}

%\def\universe{A}
%\def\Empty{\mbox{\O}}

%%%%%%%%%%%%%%%%%%% DOCUMENT %%%%%%%%%%%%%%%%%%%
\begin{document}

\title{Delayed reductions in left-to-right parsing of context-free grammars}
\author{Rehno Lindeque (23123576)\\
\textbf{Supervisors:} Derrick Kourie, Bruce Watson}

\maketitle

\begin{abstract}
In this paper we suggest the use of delayed actions for generalized parsing of context-free grammars.
Our contention is that delayed reductions in a left-to-right parser describes the nature of the parsing process in a more light-weight, intuitive manner than existing techniques.
It is also hoped that this might lead to simpler, more efficient parsers without sacrificing the generality of our grammars.
This work is therefore intended as preparatory material for further research in this area.
To this end we develop a set of tools for studying and developing the semantics of delayed parsing actions.
A small pseudo instruction set is developed with the capability to resolve production rules containing cycles in their lookahead sets.
Operational semantics for each of these instructions are provided and the implementation of an interpreter for the instruction set is described.
In addition a prototype construction algorithm is presented, capable of generating parsers for a limited subset of unambiguous grammars.
Finally we discuss the remaining problems with this technique and possible approaches to resolving them in future work.\\
\end{abstract}

\textbf{Keywords:} LD parser, recursive-ascent parser, generalized parser, context-free grammar, parser construction, bottom-up parser, top-down parser, delayed reductions

\section{Introduction}

\subsection{Top-down vs Bottom-up parsing}

The subject of parsing is widely regarded to be a well-understood topic in computer science having been exposed to many years of study.
As such, two general strategies have been identified known as top-down and bottom-up parsing.

Top-down parsing techniques operate by deriving productions near the root of the grammar before attempting to derive productions further down the tree.
Such top-down parsers are often programmed by hand using a methodology called ``recursive descent'' parsing.
A procedure is created for every production in the grammar and child productions are derived by calling their procedures in a recursive fashion.
This may be done in a speculative manner, backtracking where necessary, or by using some form of \emph{lookahead} mechanism where lexical tokens are investigated in advance before a node is created.

Bottom-up techniques on the other hand attempt to deduce the leaf nodes of the parse tree first before moving up to the root nodes. 
Theoretically this gives such a parser access to more contextual information %[citation needed] 
as the derivation of productions are delayed until their contents has already been recognized.
However, a top-down parser may not always have enough information to derive a production even when the contents of such a production has been recognized. 
Certain context-free grammars require tokens following the the production to be recognized before it can decide whether the choice is really correct inside the context in which it is recognized\footnote{This concept should not be confused with the class of grammars known as \emph{context-sensitive} grammars which may have more than one token on the left-hand side of production rules.}.
For this reason top-down parsers also typically employ lookahead in making such deductions.\\

In \cite{knuth65} Knuth developed a body of theoretical knowledge around the concept of ``left-to-right parsers with rightmost derivation'' known in short as \emph{LR parsers}. 
LR parsing captures the idea of directional bottom-up parsing with a concrete algorithm.
Over time many variants of LR parsing, such as LALR and SLR \cite{362625}, have been developed as well as some optimizations \cite{13326}.
When classifying a grammar for LR parsing we usually refer to an $LR(k)$ grammar where $k$ indicates some fixed \emph{lookahead} value, allowing the parser to look forward in the stream of input tokens when deciding on the next action to take.
However, the traditional approach to generating LR parsers with $k > 2$ has proven prohibitively difficult, leading most tools that employ LR or LALR techniques to restrict lookahead to very small values\footnote{Typical lookahead mechanisms used in LR(k) or LALR(k) parser generators have $k = 1$ or $k = 2$.}.
Despite this limitation these tools produce some of the most efficient parsers and remain a popular choice amongst users.

LL parsers or ``left-to-right parsers with left-most derivation'' are the directional top-down analog to LR parsers.
Similar to $LR(k)$ grammars, a grammar may also be classified in the class \emph{LL(k)}.
Once again $k$ indicates the minimum number of tokens that an LL parser would need to look ahead in order to recognize all the strings generated by this grammar.\\

Recursive ascent parsing is considered to be an analog to recursive descent for bottom-up parsing \cite{47909}.
In a similar manner procedures corresponding to productions are called recursively and every procedure returns the production it deduced to its caller, allowing the caller to decide on its next action.
Recursive ascent has been advocated as a learning exercise for teaching LR parsing \cite{770849}.
The parsing strategy presented in this paper, although somewhat different from a technical point of view, has some commonality with recursive ascent parsing.

\subsection{Generalized parsing approaches}

Several techniques have shown promise in delivering more general parsing approaches.
Specifically context-free languages, which include grammars with unbounded lookahead and ambiguous grammars, has seen plenty of research.
Unfortunately determining whether a context-free grammar is ambiguous, or even whether it has some unspecified upperbound of $k$ in $LR(k)$ is undecidable in general\cite{knuth65}.

Generalized parsing techniques are most often used in the domain of natural language processing while programming language tools have traditionally stuck to LR, LALR and certain LL parsing tools.
It has been shown that the strings produced by any LR(k) grammar (for some fixed k) can be recognized using an LR(1) grammar\cite{knuth65}.
However, as is suggested in \cite{Parr95lland}, a distinction can be made between \emph{language recognition} and \emph{language translation}.
Indeed, tool writers are usually more interested in processing the contents of a string rather than merely checking whether such a string matches the given grammar.
Some parser generators will even simplify the parse tree output by removing nodes that contain only a single child. [citation needed]
Furthermore, many parsers allow the user to specify additional precedence rules because writing a correct $LR(k)$ grammar can be difficult for some languages.

It has also been claimed that computer languages should be designed for weaker parsers as this makes it easier to read and understand for a human reader.
We will neither praise nor challenge this wisdom in our paper although we do suggest that further research should be conducted in order to support such a claim.\\ %\emph{todo: citation - read up about this}.\\

Given that $LR(k)$ parsers are more powerful than their equivalent $LL(k)$ parsers, larger values of $k$ are often desirable in practical LL parsers.
However, $LL(k)$ parsing, long believed to be intractable for large values of $k$, has shown large improvements in performance and generality through the use of deterministic finite automata.%\emph{todo: check this} 
The popular ANTLR parser presented the now famous variant $LL(*)$ \cite{193241}. %[citation needed] and so called Predicated-LL \cite{} [citation needed].

However, the two best known algorithms that have achieved full generality for context-free languages (aside from simple back-tracking algorithms) is the Earley algorithm \cite{362035} and Generalized LR or GLR algorithm\footnote{Sometimes the algorithm is also refered to using the name of its inventor, Tomita.} \cite{980564, 1623625}.
%\emph{todo: Both of these can be represented as chart parsers}

Despite significant improvements in the efficiency of both Earley and GLR parsers \cite{1146810, Mcpeak04elkhound:a}, they are still regarded as slow in comparison to their traditional LALR counterparts despite having similar asymptotic performance.
In GLR this is usually attributed to the cost of maintaining the stack-structured graph that gives the parser the capability to merge different branches together \cite{Mcpeak04elkhound:a}.

Other previous work similar to ours include \cite{1287949, Marc80, 991520, 146993, DBLP:conf/wia/GalvezSF06}. %[\emph{todo}].

%\emph{(TODO)\\ %BUSY HERE
%Generalized Parsing Techniques \cite{Thurston07}\\
%Chart parsers: Earley $\rightarrow$ GLR \\
%Right Nulled GLR Parsers \cite{1146810}\\
%Performance of GLR Parser \cite{Mcpeak04elkhound:a}\\ %(Same asymptotic performance, but slower by about a factor of 10x according to \cite{Mcpeak04elkhound:a} due to complicated data structures (Stack Structured Graph)
%%(todo: find a better reference?))\\
%Previous work on delayed reductions (similar ideas, but rather different algorithms) \cite{1287949, Marc80, 991520, 146993}\\
%%TODO:          Shift-Resolve Parsing: Simple,
%%     Unbounded Lookahead, Linear Time∗
%%Jos ́ Fortes G ́lvez
%%   e            a           Sylvain Schmitz            Jacques Farr ́e
%%         Universidad de Las Palmas de Gran Canaria, Spain
%%                        jfortes@dis.ulpgc.es
%%     Laboratoire I3S, Universit ́ de Nice - Sophia Antipolis, France
%%                               e
%%          schmitz@i3s.unice.fr, Jacques.Farre@unice.fr
%Parsing with continuations
%}


\subsection{Overview}
This remainder of this paper will be structured as follows. 
We begin by describing the basic ideology behind using delayed parsing actions along with some basic examples of their utility.
In the course of this section, basic operational semantics for a small, domain specific instruction set is provided.
Additional pseudo-code is given to clarify with concrete examples.

Next the theoretical aspects of the LD parsing is explored.
Discourse begins with some example traces exploring the application of delayed actions in various grammar configurations. 
Finally more formalized trace semantics are defined in preparation for future proofs.

In the final sections of the paper, the implementation details of a parser driver and a protoype parser generator are discussed.
Our findings are presented in the Results and Conclusions sections along with an overview of the topics what remain for the future work.

\section{Delayed reductions}

\subsection{Basic strategy}

There are three aspects of the parsing process that we are interested in and we will refer to each aspect by the following names.
\begin{itemize}
\item Recognition---The ability of a parser to recognize various sequences of tokens satisfying some grammar. By this definition recognition is not required to provide any output other than an assertion of the correctness of the syntax and possibly an indication of where an error might be located in the source.
\item Translation---In the context of parsing translation refers to the process of converting the source program from its original representation to the final output of the parser. The output of the parser is usually in the form of a \emph{concrete syntax tree} also commonly referred to as a \emph{parse tree}.
\item Construction---The construction of a parser is handled by a separate algorithm responsible for automatically generating executable parsing code from the static grammar definition supplied by the user. A \emph{parser generator} tool will employ this algorithm before-hand in order to generate the parser.
\end{itemize}

The \emph{grammar} of a language is a set of \emph{production rules} that is capable of generating any string in the language. 
Given a parser for such a grammar, this operation is reversed in order to reduce some string into concrete syntax tree. 
The leaf nodes in such a tree correspond directly with the lexical tokens in the input stream whereas nodes higher up in the tree are often used to add semantic value to those nodes that are lower down.
Two types of tokens are used to define a grammar:
\begin{itemize}
\item \emph{Terminal tokens} correspond to leaf nodes in the syntax tree. Notationally, terminal tokens are left uncapitalized. A special terminal, $\$$, is used to represent the unique `end-of-stream' token that indicates that no more tokens remain in the input stream.
\item \emph{Nonterminal tokens} correspond to non-leaf nodes in the syntax tree. Nonterminals are capitalized in order to distinguish them from terminals.
\end{itemize}
The rules in the grammar will be a set of strings in the form $A_1 \rightarrow x B \textellipsis$ where some finite number of terminals and nonterminals on the right-hand side are \emph{translated} to the nonterminal on the left-hand side.
At times we may also refer to the idea that the nonterminal on the left-hand side generates all the strings in the language that can be matched to the right-hand side.
A \emph{production} is the set of all rules that translate to the same nonterminal on their left-hand sides.
In our work, whenever a nonterminal token is annotated with a numeric subscript, such as $A_1$ above, it identifies the token as the nonterminal recognized from a specific rule.\\

Our basic strategy for parsing strings of lexical tokens proceeds in two passes.
The first is a \emph{recognition pass} with the additional constraint that rules are recorded as their matching strings are recognized.
The identifying rule tokens are placed into a temporary \emph{rules buffer} that grows in a stack-like manner, but also supports constant-time random-access operations.
In the second pass---the \emph{translation pass}---the sequence of recognized production rules are used to build the final parse tree in a similar a manner to a LR parser, but without any recognition steps or lookahead.

By separating these two aspects into distinct passes, the recognition pass is allowed the convenience of identifying the right-hand sides of production rules in a more flexible order. 
However, the translation pass requires the rules buffer it is given to be correctly ordered when building the parse tree.
For this purpose, the mechanism used to reorder rules relies on placeholder tokens in the rules buffer representing unresolved production rules.
Whenever the parser must deduce a rule that cannot be fully determined, a placeholder token is appended to the rules buffer in its place. 
Once the rule can be resolved, the placeholder token is replaced by the correct rule token.
In order to keep track of placeholders their indexes are recorded in a separate stack, allowing the parser to return to these tokens later on and replace them using random-access writes.\\

In effect, placeholders are used in order to delay reductions in an LR-like parser until some unspecified lookahead has been recognized.
Clearly the order of rule recognition is relaxed using this mechanism.
However, it is not completely unrestricted.
The following conditions specify certain constraints by which the recognition pass must abide.

\begin{itemize}
\item As rules are recognized they are placed in the rules buffer in a left-to-right order. This corresponds with a bottom-up parsing strategy since production rules that are closer to the leaf nodes in the parse tree will normally be recognized before their parent rules are recognized.
\item Delayed rules may only be resolved in a right-to-left order, corresponding with the top-down strategy of parsing since delayed rules near the root of the tree must be resolved before 
rules near the leaf nodes can be resolved. 
It is worth noting that, although the unresolved rules will only be resolved in a right-to-left order, rules may be added to the unresolved stack at any time.
Intuitively the right-to-left rule resolution loop is nested inside the main left-to-right recognition loop, to be evaluated whenever the last delayed rule's lookahead has been completed.
\end{itemize}

In order to explain this strategy in more formal terms we will define an instruction set and interpreter which in essence forms a tiny domain specific language for parsing with delayed actions.
For the sake of simplicity we begin by describing a simple modified LR(0) algorithm. This LR(0) algorithm has three simple operations: $shift$, $reduce$ and $switch$. 
The $shift$ and $switch$ operations are responsible for recognizing lexical tokens on the input stream whereas the $reduce$ operation is responsible for translating strings of terminal tokens into nonterminal tokens.

The operation of a parser constructed as an interpreter for the given set of instructions is demonstrated.
The following free variables will be used to represent the state of the interpreter during execution:

\begin{itemize}
\item $input_{[0, input.length)}$ is an array of input tokens of length $input.length$ returned by a lexer.
\item $i$ is an index into the array of input tokens representing the current position of the parser in the stream during the recognition pass such that $input_i$ is the token currently being tested.
This index will never be decreased and need only increase in increments of 1.
\item $actions$ is a sequence of instructions representing the encoded parser similar to the source code that is output by parser generators. 
This array is automatically generated by a separate parser construction algorithm.
\item $j$ is an index into the array of actions such that $actions_j$ refers to the current instruction being decoded.
\item $outputrules$ is a dynamically growing array storing the output of recognition pass.
\item $k$ is an index into the array of output rules such that $k$ always points to the first unfilled element of the array.
\item $callstack$ is a dynamically growing stack for storing the current index of the parser in the instruction table \emph{todo: instruction stream} in order to return to it later.
\item $l$ is an index into the call stack such that $l$ always points to the first unfilled element of the stack.
%\item $continuations$ is a stack for storing the current index of the parser in the instruction table in order to return to it later.
\item $error$ is a special error state which (in the absence of error recovery) will halt the parser and report a syntax error.
\end{itemize}

Every instruction in the set will be represented using the format\\ \texttt{ operation [( [parameters\textellipsis] )]}\\

Operational semantics for the following instructions are discussed below, followed by the implementation of a basic $LR(0)$ parser.
\begin{itemize}
\item \texttt{shift(terminaltoken)}
\item \texttt{switch(terminaltoken $\mapsto$ state, \textellipsis)}
\item \texttt{reduce(rule)}
\end{itemize}

\subsubsection{The `shift' operation}
\texttt{shift(terminaltoken)}\\
The $shift$ action is used to step over input tokens deterministicly. 
There are only two options: Either the token matches the parameter given to the $shift$ operation, or the parser is put into an error state.

\begin{equation}
\infer{i, j \mapsto i+1, j+1}{input_i = a & actions_j = shift(a)} \tag{Def 1.1}
\end{equation}

\begin{equation}
\infer{parserstate \mapsto error}{input_i = a & actions_j = shift(b)} \tag{Def 1.2}
\end{equation}\\

\subsubsection{The `switch' operation}
\texttt{switch(terminaltoken $\mapsto$ state, \textellipsis)}\\
The $switch$ instruction is used to recognize input tokens nondeterministicly.
The set of parameters given to the $switch$ operation can be viewed as an associative map of key-value pairs where a terminal token maps to a parse state or, in other words, an index into the generated parser code.
If a terminal token on the input stream is contained in the map then the $switch$ operation will cause the parser to jump to the new state similar to how switch statements behave in many programming languages.
If the input terminal is not recognized by the $switch$ action, then the parser is placed in the $error$ state. 
In the definition below the function $getvalue$ returns the state index value corresponding to the terminal token given as the key.

\begin{equation}
\infer{i, j \mapsto i+1, getvalue(A, input_i)}{input_i \in A & actions_j = switch(A)} \tag{Def 1.3}
\end{equation}

\begin{equation}
\infer{parsestate \mapsto error}{input_i \notin A & actions_j = switch(A)} \tag{Def 1.4}
\end{equation}\\

There is an apparent redundancy between the switch and the shift actions. 
A $shift$ instruction of the form $shift(a)$ will behave similar to a $switch$ instruction in the form $switch([a \mapsto next])$ where $next$ indicates the index of the instruction that directly follows the $switch$.
However, a distinction is made between these two actions as a matter of convenience: 
We view $switch$ operations as the primary recognition mechanism of the parser, because they involve a non-trivial choice.
By contrast, $shift$ operations do not make any significant contribution to the internal state of the parser other than to catch syntax errors.
	
\subsubsection{The `reduce' operation}
\texttt{reduce(rule)}\\
The parameter given to the $reduce$ instruction is an index into a list of all rules in the given grammar where every rule is in the form \texttt{nonterminaltoken $\rightarrow$ terminaltoken \textellipsis}
A production in a grammar may be reduced from any number of different sequences of terminal tokens, however a rule recognizes only one possible sequence of terminal tokens.
In other words, given some rule both the left and right side of the reduction can be determined (although identifiers must still be stored).
For this reason we use rule numbers internally rather than using productions directly.

\begin{equation}
\infer{j, outputrules_k, k \mapsto j+1, r, k+1}{actions_j = reduce(r)} \tag{Def 1.5}
\end{equation}\\

\subsubsection{Basic LR(0) parsing}

Since LR(0) grammars requires no lookahead they can be parsed directly using only recognition and reduction actions.
To see how these rules translate into a familiar language we construct an LR(0) parser for a simple grammar in the guarded command language (GCL) in a similar style to a recursive-ascent parser.

This simple grammar will be used for illustration, with $S$ representing the starting token.

\begin{align*}
G \equiv \quad & A_1 \rightarrow a\\
               & B_1 \rightarrow b\\
               & S_1 \rightarrow x A \$\\
               & S_2 \rightarrow x B \$\\
               & S_3 \rightarrow x y \$
\end{align*}

In order to recognize this grammar we will derive the following sequence of parsing steps:
\begin{enumerate}
\item The first token to be recognized must be $x$. Since the choice of $x$ is predetermined a shift action, $shift(x)$, may be used which also checks for an error in the input stream.
%\begin{figure}[htbp]
\begin{center}
\begin{gcl}
\IF input_i = x \rightarrow i \becomes i + 1;
\BAR input_i \neq x \rightarrow parserstate \becomes error;
\FI
\end{gcl}
\end{center}
%\end{figure}

\item Now the set of valid tokens to follow are $\{ a, b, y \}$. 
Since this recognition involves a choice of $a$, $b$ or $y$ a logical branch occurs in the parser, indicating that a $switch$ action must be employed.
The choice of branch will place the parser into one of three distinct states. Hence it makes sense to indicate this by constructing a separate procedure for each.
%\begin{figure}[htbp]
\begin{center}
\begin{gcl}
\IF input_i = a \rightarrow 
                \qquad i \becomes i+1; 
                \qquad ParseS1(input, i);
\BAR input_i = b \rightarrow 
                \qquad i \becomes i+1;
                \qquad ParseS2(input, i);
\BAR input_i = y \rightarrow 
                \qquad i \becomes i+1; 
                \qquad ParseS3(input, i);
\BAR input_i \notin \{ a, b \} \rightarrow 
                \qquad parserstate \becomes error;
\FI
\end{gcl}
\end{center}
%\end{figure}

\item For each branch of the switch the remaining parsing actions are fully deterministic as no more choices remain.
The first two branches must output their corresponding reduction rules $A_1$ or $B_1$. 
For example, the reduction $reduce(A_1)$ may be performed as follows:
%\begin{figure}[htbp]
\begin{center}
\begin{gcl}
outputrules_k \becomes A_1;
k \becomes k + 1;
\end{gcl}
\end{center}
%\end{figure}

\item Now all that remains in each branch is to recognize the remaining `end-of-stream' token $\$$ and then output $S_1$, $S_2$ or $S_3$. 
The complete pseudo code that describes this process is listed below.

\begin{center}
\begin{gcl}
\PROC ParseS1(input)
outputrules_k \becomes A_1;
k \becomes k + 1;
\IF input_i = \$ \rightarrow 
                 \qquad i \becomes i + 1;
\BAR input_i \neq \$ \rightarrow 
                 \qquad parserstate \becomes error;
                 \qquad \textbf{return};
\FI
outputrules_k \becomes S_1;
k \becomes k + 1;
\CORP
\end{gcl}
\end{center}

\begin{center}
\begin{gcl}
\PROC ParseS2(input)
outputrules_k \becomes B_1;
k \becomes k + 1;
\IF input_i = \$ \rightarrow i \becomes i + 1;
\BAR input_i \neq \$ \rightarrow
                 \qquad parserstate \becomes error;
                 \qquad \textbf{return};
\FI
outputrules_k \becomes S_2;
k \becomes k + 1;
\CORP
\end{gcl}
\end{center}

\begin{center}
\begin{gcl}
\PROC ParseS3(input)
\IF input_i = \$ \rightarrow i \becomes i + 1;
\BAR input_i \neq \$ \rightarrow 
                 \qquad parserstate \becomes error;
                 \qquad \textbf{return};
\FI
outputrules_k \becomes S_3;
k \becomes k + 1;
\CORP
\end{gcl}
\end{center}

\clearpage
\begin{figure}[!ht]
\begin{center}
\begin{gcl}
\PROC ParseG(input, output, parserstate)
i \becomes 0;
\IF input_i = x \rightarrow i \becomes i + 1;
\BAR input_i \neq x \rightarrow 
                \qquad parserstate \becomes error;
                \qquad \textbf{return};
\FI
\IF input_i = a \rightarrow
                \qquad i \becomes i+1;
                \qquad ParseS1(input, i);
\BAR input_i = b \rightarrow 
                \qquad i \becomes i+1;
                \qquad ParseS2(input, i);
\BAR input_i = y \rightarrow 
                \qquad i \becomes i+1;
                \qquad ParseS3(input, i);
\BAR input_i \notin \{ a, b \} \rightarrow 
                \qquad parserstate \becomes error; 
                \qquad \textbf{return};
\FI
\CORP
\end{gcl}
\caption{An LR(0) parser example explained in pseudo code.}
\end{center}
\end{figure}

\end{enumerate}

\subsection{Delayed actions}
While the three operations discussed provide the necessary tools to recognize any non-recursive LR(0) grammar, they do not provide any manner of looking ahead into the input stream. Furthermore parsers using these instruction will exhibit exponential growth if no explicit `return' action is available.
Some additional actions are provided to resolve these issues:
\begin{itemize}
\item \texttt{delay}
\item \texttt{resolve(rule)}
\item \texttt{return}
\item \texttt{goto(originstate $\mapsto$ targetstate, ...)}
\item In addition the \texttt{switch} action is slightly modified to use the callstack.
\end{itemize}

Some additional free variables is needed for use by the extended instruction set.

\begin{itemize}
\item $delays$ is a stack of indexes into the rules 
\item $m$ is an index into $delays$ such that $delays_m$ always refers to the first unfilled element of the stack.
\item $lasttarget$ is a index into the instruction stream recorded by the $switch$ operation as described bellow.
\end{itemize}

\subsubsection{The `delay' operation}
\texttt{delay}\\
The delay action adds a placeholder token called $ignore$ to the output rules which may be replaced later.
In addition the index of the placeholder is pushed onto the stack of delays.
Note that the value of $ignore$ should be reserved so that it will not conflict with any rule number.
\begin{equation}
\infer{delays_m, m, j, outputrules_k, k \mapsto j, m+1, j+1, ignore, k+1}{actions_j = delay} \tag{Def 1.6}
\end{equation}\\

\subsubsection{The `resolve' operation}
\texttt{resolve(rule)}\\
The resolve action is similar to the reduce action, but instead of immediately pushing a reduction rule onto the output stack it instead replaces the placeholder token previously pushed by a delay action.
It may also push the special token $ignore$ meaning that the output rules should remain unchanged and no reduction is necessary. 
Since the resolve action will pop the index of this placeholder token from the stack, this delay would not be re-evaluated again.
\begin{equation}
\infer{l, j, outputrules_{delays_m}, m \mapsto l-1, j+1, ignore, m-1}{actions_j = resolve(r)} \tag{Def 1.7}
\end{equation}\\

The pair of actions, `resolve' and `delay', works together in order to implement \emph{delayed reductions}. 
It is easy to see that the action $delay$ immediately followed by the action $resolve(r)$ is identical to our original action $reduce(r)$ and could be used as a replacement\footnote{We've chosen to keep the original reduce action because, in practice, programming language grammars often require very few lookaheads.}.

\subsubsection{A modification of the `switch' operation}
In order for the nexed two operations to work correctly we must also modify our original $switch$ action to record the state from which we've come.
The current parser state index will be pushed onto a call stack to be returned to later. 
However, the $switch$ also sets one additional variable, $lasttarget$ which always contains the very last state that any $switch$ instruction jumped to.

\begin{equation}
\infer{i, j, lasttarget, callstack_l, l \mapsto i+1, getvalue(A, input_i), getvalue(A, input_i), j+1, l+1}{input_i \in A & actions_j = switch(A)} \tag{Def 1.8}
\end{equation}

However error checking remains the same as before.
\begin{equation}
\infer{parsestate \mapsto error}{input_i \notin A & actions_j = switch(A)} \tag{Def 1.9}
\end{equation}

\subsubsection{The `return' operation}
A $return$ action is needed in order to return to the state from which the previous $switch(...)$ action jumped. 
From a technical point of view, this allows a parser constructor to merge branches together, thus avoiding an exponential growth in the number of possible states. 
Indeed this is a necessary condition for parsers that must handle recursive grammars.

\begin{equation}
\infer{j, l \mapsto callstack_l, l-1}{actions_j = return} \tag{Def 1.10}
\end{equation}

\subsubsection{The `goto' operation}
%Note: goto is a type of "trampoline"?
One final operation is needed to complete the instruction set.
When a cyclic set of rules has been recognized, then the $delay$ operations triggered during each iteration of the cycle must also be resolved in a recursive manner.
The only way to do this is by using the $return$ operation to revisit each of these states.
However, some kind of history is needed to retain knowledge of all of the lookahead that has been recognized.

A $goto$ operation can be described as type of `trampoline'. 
Immediately after returning from another state, the interpreter may encounter a $goto$ instruction which will cause the interpreter to jump to a different state.
Depending on the last value assigned to $lasttarget$ by a $switch$ operation, $goto$ will direct the parser to some associated state. 
The variable $lasttarget$ is representative of all the relevant branches that that the parser needed to follow in order to recognize the needed lookahead up to this point.
In other words the last nondeterministic choice in a sequence of nondeterministic choices will be used to represent the entire sequence of choices.

\begin{equation}
\infer{j \mapsto getvalue(A, lasttarget)}{lasttarget \in A & actions_j = goto(A)} \tag{Def 1.11}
\end{equation}

The $goto$ operation does not otherwise affect the callstack or the the value of $lasttarget$. 
If no other $switch$ instruction is encountered then the next $return$ will continue with the previous state on the callstack.

\subsection{Traces}
To see how the delay/resolve pair can be used to look ahead in the input stream we must investigate several different forms of production rules.
In order to do this we must generate all possible input strings for each form.
The concept of traces is used to show the actions that the parser must take for every possibility.
A trace is simply a sequence of actions that the parser must take to recognize a particular instance of a valid input stream for the given grammar.
The advantage of this approach is that control flow can be ignored in order to focus exclusively on analysing which permutations of parsing actions produce equivalent output.
A mechanical method of generating traces is used such that every possibility could be enumerated, although one should be aware that the trace itself may not be finite.\\

%%%%%%%%%%%%% Example 1: Reductions required a fixed size lookahead
\subsubsection{Reductions requiring a fixed size lookahead}
\begin{tabular}[t]{cl}
\parbox{.3\textwidth}{
\begin{align*}
G_1 \equiv \quad & A_1 \rightarrow x\\
                 & B_1 \rightarrow x\\
                 & S_1 \rightarrow A a \$\\
                 & S_2 \rightarrow B b \$
\end{align*}}
\parbox{.6\textwidth}{This is a simple grammar requiring a fixed size lookahead in order to output one of the two production rules, $A_1$ or $B_1$.}
\end{tabular}

Suppose that this grammar was to be parsed by an $LR(k)$ parser with a sufficiently large lookahead $k$. 
We could write down all the possible traces for this parser using our `shift', `reduce' and `switch' actions while ignoring any possible lookahead actions that must take place as follows:

{\small\parbox{.25\textwidth}{\begin{align*}
&traces(G_1) \equiv \\
\{ & \langle shift(x), reduce(A_1), switch(a), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), reduce(B_1), switch(b), shift(\$), reduce(S_2) \rangle \}
\end{align*}}}

Next we will exploit the fact that ordering of shift and switch actions relative to reduce actions is not enforced in our parsing methodology. This because a parse tree is only constructed during the second pass over the output reduction rules.
This really implies that any recognition action directly followed by a reduce action may swap places. 
Hence a trace $\langle shift(...), reduce(...) \rangle$ may be rewritten as $\langle reduce(...), shift(...) \rangle$ and similarly for switch statements $\langle switch(...), reduce(...) \rangle$ may be rewritten as $\langle reduce(...), switch(...) \rangle$.
We apply this rule selectively to the above trace in order to produce an equivalent trace:

{\small\parbox{.3\textwidth}{\begin{align*}
&traces(G_1) \equiv \\
\{ & \langle shift(x), switch(a), reduce(A_1), shift(\$), reduce(S_1) \rangle, \\
   & \langle shift(x), switch(b), reduce(B_1), shift(\$), reduce(S_2) \rangle \}
\end{align*}}}

Take note that this equivalent trace requires no lookahead whatsoever\footnote{Note that an $LR(1)$ parser is effectively allowed to perform one such swap for every $reduce$ operation.}!
Because the ordering of recognition and reduction operations have been relaxed many instances where a fixed lookahead would have been required by classical algorithms no longer apply.

%%%%%%%%%%%%% Example 2: Optional reduction based on a fixed lookahead
\subsubsection{Optional reduction requiring a fixed size lookahead}
\begin{tabular}[t]{cl}
\parbox{.3\textwidth}{
\begin{align*}
G_2 \equiv \quad & A_1 \rightarrow x\\
                 & S_1 \rightarrow A a \$\\
                 & S_2 \rightarrow x b \$
\end{align*}}
\parbox{.6\textwidth}{In this grammar the rule $A_1$ may be output, or the possible reduction could be ignored if $b$ is in the lookahead.}
\end{tabular}

This grammar produces a similar trace as before, but with the first reduction omitted in the rule $S_2$.

{\small\parbox{.3\textwidth}{\begin{align*}
&traces(G_2) \equiv \\
\{ & \langle shift(x), reduce(A_1), switch(a), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), switch(b), shift(\$), reduce(S_2) \rangle \}
\end{align*}}}

Once again the need for lookahead can be circumvented by simply shuffling the order of operations.

{\small\parbox{.3\textwidth}{\begin{align*}
&traces(G_2) \equiv \\
\{ & \langle shift(x), switch(a), reduce(A_1), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), switch(b), shift(\$), reduce(S_2) \rangle \}
\end{align*}}}

%%%%%%%%%%%%% Example 3: Disjoint reductions that require lookahead
\subsubsection{Disjoint reductions that require lookahead}
Reductions in different branches of a grammar's rules do not necessarily consume the same number of tokens. 
In addition the prefix of their recognized token strings may be the same, which requires a lookahead to be performed.\\
\begin{tabular}[t]{cl}
\parbox{.3\textwidth}{
\begin{align*}
G_3 \equiv \quad & A_1 \rightarrow x y\\
                 & B_1 \rightarrow y b\\
                 & C_1 \rightarrow x\\
                 & S_1 \rightarrow A a \$\\
                 & S_2 \rightarrow C B \$
\end{align*}}
\parbox{.6\textwidth}{The rules $A_1$ and $C_1$, $B_1$ in this grammar are partially disjoint.}
\end{tabular}

Once the lexical token $x$ has been seen, the parser must first look ahead 2 tokens before 
deciding whether the string $x y$ belongs to $A_1$ or to $C_1$ followed by $B_1$.

{\small\parbox{.3\textwidth}{\begin{align*}
&traces(G_3) \equiv \\
\{ & \langle shift(x), shift(y), reduce(A_1), switch(a), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), reduce(C_1), shift(y), switch(b), reduce(B_1), shift(\$), reduce(S_2) \rangle \}
\end{align*}}}

This trace may also be reordered without any need for lookahead.

{\small\parbox{.3\textwidth}{\begin{align*}
&traces(G_3) \equiv \\
\{ & \langle shift(x), shift(y), switch(a), reduce(A_1), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), shift(y), switch(b), reduce(C_1), reduce(B_1), shift(\$), reduce(S_2) \rangle \}
\end{align*}}}

%%%%%%%%%%%%% Example 4: Right recursive reductions with outer lookahead
\subsubsection{Look ahead outside a right recursive cycle}
In the previous examples, the traces could simply be reordered in order to obviate the need for lookahead, however this is no longer possible in the presence of recursive production rules.
Such a recursive grammar with a lookahead outside of its cycle no longer falls into the class of LR(k) grammars since k is unbounded in these instances.

\begin{tabular}[t]{cl}
\parbox{.3\textwidth}{
\begin{align*}
G_4 \equiv \quad & A_1 \rightarrow x\\
                 & A_2 \rightarrow x A\\
                 & B_1 \rightarrow x\\
                 & B_2 \rightarrow x B\\
                 & S_1 \rightarrow A a \$\\
                 & S_2 \rightarrow B b \$
\end{align*}}
\parbox{.6\textwidth}{The rules $A_2$ and $B_2$ in this grammar are both in a form of \emph{tail recursion}. 
We will also call this \emph{right recursion} in this paper in order to emphasize the contrast with \emph{left recursion}.}
\end{tabular}

{\small\parbox{.3\textwidth}{\begin{align*}
&traces(G_4) \equiv \\
\{ & \langle shift(x), reduce(A_1), switch(a), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), shift(x), reduce(A_1), reduce(A_2), switch(a), shift(\$), reduce(S_1) \rangle, \textellipsis \\
   & \langle shift(x), reduce(B_1), switch(b), reduce(S_2) \rangle,\\
   & \langle shift(x), shift(x), reduce(B_1), reduce(B_2), switch(b), shift(\$), reduce(S_2) \rangle, \textellipsis \}
\end{align*}}}

While we could attempt to shuffle the operations in these traces so that reductions are only made after the tokens $a$ and $b$ have been recognized, 
additional effort would be required on the part of the parser to remember the number and nature of reductions to apply. 
By using a $return$ action we can determine which rules should be output, however the rules should also be applied in the correct order to produce the correct trace. 
We will discuss this more fully in the following section.
Suffice to say, reductions could be recognized in a reversed (top-down) order but needs to be output in the original (bottom-up) ordering.
However our $delay/resolve$ mechanism will allow us to do exactly this by placing placeholders in the output stream and later replacing them in right-to-left order using $resolve$ actions.
Hence, the equivalent trace can be written as follows:

{\small\parbox{.3\textwidth}{\begin{align*}
&traces(G_4) \equiv \\
\{ & \langle shift(x), delay, switch(a), resolve(A_1), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), shift(x), delay, delay, switch(a), resolve(A_2), resolve(A_1), shift(\$), reduce(S_1) \rangle, \textellipsis \\
   & \langle shift(x), delay, switch(b), resolve(B_1), shift(\$), reduce(S_2) \rangle,\\
   & \langle shift(x), shift(x), delay, delay, switch(b), resolve(B_2), resolve(B_1), shift(\$), reduce(S_2) \rangle, \textellipsis \}
\end{align*}}}

Notice the reverse ordering of the actions $\langle resolve(A_2), resolve(A_1) \rangle$ in contrast to the original $\langle reduce(A_1), reduce(A_2) \rangle$.
A left recursive rule with outer lookahead may be resolved in much the same manner as a right recursive rule.

%%%%%%%%%%%%% Example 5: Combined left and right recursive grammar
\subsubsection{Look ahead outside a combined left and right recursive cycle}
\begin{tabular}[t]{cl}
\parbox{.3\textwidth}{
\begin{align*}
G_5 \equiv \quad & A_1 \rightarrow x\\
                 & A_2 \rightarrow x A\\
                 & B_1 \rightarrow x\\
                 & B_2 \rightarrow B x\\
                 & S_1 \rightarrow A a \$\\
                 & S_2 \rightarrow B b \$
\end{align*}}
\parbox{.6\textwidth}{The rules $A_2$ and $B_2$ are both partially disjoint and recursive. One is left recursive in this case while the other is right recursive and the combination
requires a look ahead outside of the cycle.}
\end{tabular}

{\small\parbox{.3\textwidth}{\begin{align*}
&traces(G_5) \equiv \\
\{ & \langle shift(x), delay, switch(a), resolve(A_1), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), shift(x), delay, delay, switch(a), resolve(A_2), resolve(A_1), shift(\$), reduce(S_1) \rangle, \textellipsis \\
   & \langle shift(x), delay, switch(b), resolve(B_1), shift(\$), reduce(S_2) \rangle,\\
   & \langle shift(x), shift(x), delay, delay, switch(b), resolve(B_2), resolve(B_1), shift(\$), reduce(S_2) \rangle, \textellipsis \}
\end{align*}}}

An equivalent trace with delays can be written using delay/resolve actions as follows:

{\small\parbox{.3\textwidth}{\begin{align*}
&traces(G_5) \equiv \\
\{ & \langle shift(x), delay, switch(a), resolve(A_1), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), shift(x), delay, delay, switch(a), resolve(A_2), resolve(A_1), shift(\$), reduce(S_1) \rangle, \textellipsis \\
   & \langle shift(x), delay, switch(b), resolve(B_1), shift(\$), reduce(S_2) \rangle,\\
   & \langle shift(x), shift(x), delay, delay, switch(b), resolve(B_2), resolve(B_1), shift(\$), reduce(S_2) \rangle, \textellipsis \}
\end{align*}}}

%%%%%%%%%%%%% Example: 
%\subsubsection{}
%\begin{tabular}[t]{cl}
%\parbox{.3\textwidth}{
%\begin{align*}
%G_6 \equiv \quad & A_1 \rightarrow x\\
%                 & B_1 \rightarrow x\\
%                 & C_1 \rightarrow y\\
%                 & C_2 \rightarrow A C a\\
%                 & C_3 \rightarrow B C b\\
%                 & S_1 \rightarrow C \$
%\end{align*}}
%\parbox{.6\textwidth}{sfd}
%\end{tabular}
%
%\parbox{.3\textwidth}{\begin{align*}
%&traces(G_6) \equiv \\
%\{ & \langle switch(y), reduce(C_1), shift(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(A_1), switch(y), reduce(C_1), switch(a), reduce(C_2), shift(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(B_1), switch(y), reduce(C_1), switch(b), reduce(C_3), shift(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(A_1), switch(x), reduce(A_1), switch(y), reduce(C_1), switch(a), reduce(C_2), \\&\qquad switch(a), reduce(C_2), shift(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(A_1), switch(x), reduce(B_1), switch(y), reduce(C_1), switch(b), reduce(C_3), \\&\qquad switch(a), reduce(C_2), shift(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(B_1), switch(x), reduce(A_1), switch(y), reduce(C_1), switch(a), reduce(C_2), \\&\qquad switch(b), reduce(C_3), shift(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(B_1), switch(x), reduce(B_1), switch(y), reduce(C_1), switch(b), reduce(C_3), \\&\qquad switch(b), reduce(C_3), shift(\$), reduce(S_1) \rangle,\\
%   & \textellipsis \}
%\end{align*}}
%
%An equivalent trace with delays can be written using delay/resolve actions as follows:
%
%\parbox{.3\textwidth}{\begin{align*}
%%\begin{equation*}
%&traces(G_6) \equiv \\
%\{ & \langle switch(y), reduce(C_1), shift(\$), reduce(S_1) \rangle,\\ 
%%\begin{array}{l}
%   &  \langle switch(x), delay(A,B), switch(y), reduce(C_1), switch(a), resolve(A), reduce(C_2), shift(\$), \\&\qquad reduce(S_1) \rangle,\\
%   &  \langle switch(x), delay(A,B), switch(y), reduce(C_1), switch(b), resolve(B), reduce(C_3), shift(\$), \\&\qquad reduce(S_1) \rangle,\\
%   &  \langle switch(x), delay(A,B), switch(x), delay(A,B), switch(y), reduce(C_1), switch(a), resolve(A), \\&\qquad reduce(C_2), switch(a), resolve(A), reduce(C_2), shift(\$), reduce(S_1) \rangle,\\
%   &  \langle switch(x), delay(A,B), switch(x), delay(A,B), switch(y), reduce(C_1), switch(b), resolve(B), \\&\qquad reduce(C_3), switch(a), resolve(A), reduce(C_2), shift(\$), reduce(S_1) \rangle,\\
%   &  \langle switch(x), delay(A,B), switch(x), delay(A,B), switch(y), reduce(C_1), switch(a), resolve(A), \\&\qquad reduce(C_2), switch(b), resolve(B), reduce(C_3), shift(\$), reduce(S_1) \rangle,\\
%   &  \langle switch(x), delay(A,B), switch(x), delay(A,B), switch(y), reduce(C_1), switch(b), resolve(B), \\&\qquad reduce(C_3), switch(b), resolve(B), reduce(C_3), shift(\$), reduce(S_1) \rangle,\\
%   &  \textellipsis
%%\end{array}
%\}
%%\end{equation*}
%\end{align*}}

%%%%%%%%%%%%%% Example 7: 
%\subsubsection{}
%\begin{tabular}[t]{cl}
%\parbox{.3\textwidth}{
%\begin{align*}
%G_7 \equiv \quad & A_1 \rightarrow x\\
%                 & B_1 \rightarrow x\\
%                 & C_1 \rightarrow y\\
%                 & C_2 \rightarrow C A a\\
%                 & C_3 \rightarrow C B b\\
%                 & S_1 \rightarrow C \$
%\end{align*}}
%\parbox{.6\textwidth}{sfd}
%\end{tabular}
%
%\parbox{.3\textwidth}{\begin{align*}
%&traces(G_7) \equiv \\
%\{ & \langle shift(y), reduce(C_1), switch(\$), reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), reduce(A_1), switch(a), reduce(C_2), switch(\$), reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), reduce(B_1), switch(b), reduce(C_3), switch(\$), reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), reduce(A_1), switch(a), reduce(C_2), switch(x), reduce(A_1), \\&\qquad switch(a), reduce(C_2), switch(\$), reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), reduce(B_1), switch(b), reduce(C_3), switch(x), reduce(A_1), \\&\qquad switch(a), reduce(C_2), switch(\$), reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), reduce(A_1), switch(a), reduce(C_2), switch(x), reduce(B_1), \\&\qquad switch(b), reduce(C_3), switch(\$), reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), reduce(B_1), switch(b), reduce(C_3), switch(x), reduce(B_1), \\&\qquad switch(b), reduce(C_3), switch(\$), reduce(S_1) \rangle,\\
%   & \textellipsis \}
%\end{align*}}
%
%The equivalent trace with delays can be written as follows:
%
%\parbox{.3\textwidth}{\begin{align*}
%&traces(G_7) \equiv \\
%\{ & \langle shift(y), reduce(C_1), switch(\$), reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), delay(A,B), switch(a), resolve(A_1), reduce(C_2), switch(\$), \\&\qquad reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), delay(A,B), switch(b), resolve(B_1), reduce(C_3), switch(\$), \\&\qquad reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), delay(A,B), switch(a), resolve(A_1), reduce(C_2), switch(x), \\&\qquad delay(A,B), switch(a), resolve(A_1), reduce(C_2), switch(\$), reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), delay(A,B), switch(a), resolve(A_1), reduce(C_2), switch(x), \\&\qquad delay(A,B), switch(b), resolve(B_1), reduce(C_3), switch(\$), reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), delay(A,B), switch(b), resolve(B_1), reduce(C_3), switch(x), \\&\qquad delay(A,B), switch(b), resolve(B_1), reduce(C_3), switch(\$), reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), delay(A,B), switch(b), resolve(B_1), reduce(C_3), switch(x), \\&\qquad delay(A,B), switch(a), resolve(A_1), reduce(C_2), switch(\$), reduce(S_1) \rangle,\\
%   & \textellipsis \}
%\end{align*}}

%%%%%%%%%%%%% Example 8: 
%\subsubsection{}
%\begin{tabular}[t]{cl}
%\parbox{.3\textwidth}{
%\begin{align*}
%G_8 \equiv \quad & A_1 \rightarrow x\\
%                 & B_1 \rightarrow x\\
%                 & C_1 \rightarrow y\\
%                 & C_2 \rightarrow A C a\\
%                 & C_3 \rightarrow C B b\\
%                 & S_1 \rightarrow C \$
%\end{align*}}
%\parbox{.6\textwidth}{sfd}
%\end{tabular}
%
%\parbox{.3\textwidth}{\begin{align*}
%&traces(G_8) \equiv \\
%\{ & \langle switch(y), reduce(C_1), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(A_1), switch(y), reduce(C_1), switch(a), reduce(C_2), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(y), reduce(C_1), switch(x), reduce(B_1), shift(b), reduce(C_3), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(A_1), switch(x), reduce(A_1), switch(y), reduce(C_1), shift(a), reduce(C_2), shift(a), \\&\qquad reduce(C_2), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(A_1), switch(y), reduce(C_1), switch(x), reduce(B_1), switch(b), reduce(C_3), shift(a), \\&\qquad reduce(C_2), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(A_1), switch(y), reduce(C_1), switch(a), reduce(C_2), switch(x), reduce(B_1), shift(b), \\&\qquad reduce(C_3), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(y), reduce(C_1), switch(x), reduce(B_1), shift(b), reduce(C_3), switch(x), reduce(B_1), shift(b), \\&\qquad reduce(C_3), switch(\$), reduce(S_1) \rangle,\\
%   & \textellipsis \}
%\end{align*}}
%
%The equivalent trace with delays can be written as follows:
%
%\parbox{.3\textwidth}{\begin{align*}
%&traces(G_8) \equiv \\
%\{ & \langle switch(y), reduce(C_1), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(A_1), switch(y), reduce(C_1), switch(a), reduce(C_2), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(y), reduce(C_1), switch(x), reduce(B_1), shift(b), reduce(C_3), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(A_1), switch(x), reduce(A_1), switch(y), reduce(C_1), shift(a), reduce(C_2), shift(a), \\&\qquad reduce(C_2), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(A_1), switch(y), reduce(C_1), switch(x), reduce(B_1), switch(b), reduce(C_3), shift(a), \\&\qquad reduce(C_2), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(A_1), switch(y), reduce(C_1), switch(a), reduce(C_2), switch(x), reduce(B_1), shift(b), \\&\qquad reduce(C_3), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(y), reduce(C_1), switch(x), reduce(B_1), shift(b), reduce(C_3), switch(x), reduce(B_1), shift(b), \\&\qquad reduce(C_3), switch(\$), reduce(S_1) \rangle,\\
%   & \textellipsis \}
%\end{align*}}

%S1{ C1{y} $ }
%S1{ C2{ A{x} C1{y} a} $ }
%S1{ C3{ C1{y} B{x} b } $ }
%S1{ C2{ A{x} C2{ A{x} C1{y} a } a } $ }
%S1{ C2{ A{x} C3{ C1{y} B{x} b } a } $ }
%S1{ C3{ C2{ A{x} C1{y} a } B{x} b } $ }
%S1{ C3{ C3{ C1{y} B{x} b } B{x} b } $ }

%(Surprisingly enough, this one needs no look-aheads)


%%%%%%%%%%%%% Example 9: Look ahead over a cycle
\subsubsection{Look ahead over a cycle}
\begin{tabular}[t]{cl}
\parbox{.3\textwidth}{
\begin{align*}
G_9 \equiv \quad & A_1 \rightarrow x\\
                 & B_1 \rightarrow x\\
                 & C_1 \rightarrow y\\
                 & C_2 \rightarrow C y\\
                 & S_1 \rightarrow A C a \$\\
                 & S_2 \rightarrow B C b \$
\end{align*}}
\parbox{.6\textwidth}{In this grammar the rules $A_1$ and $B_1$ are not recursive, however their lookahead includes a cycle in $C$. 
This type of grammar also requires a delayed reductions to resolve the choice of $A_1$  $B_1$.}
\end{tabular}

{\small\parbox{.3\textwidth}{\begin{align*}
&traces(G_9) \equiv \\
\{ & \langle shift(x), reduce(A_1), shift(y), reduce(C_1), switch(a), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), reduce(B_1), shift(y), reduce(C_1), switch(b), shift(\$), reduce(S_2) \rangle,\\
   & \langle shift(x), reduce(A_1), shift(y), reduce(C_1), switch(y), reduce(C_2), switch(a), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), reduce(B_1), shift(y), reduce(C_1), switch(y), reduce(C_2), switch(b), shift(\$), reduce(S_2) \rangle,\\
   & \textellipsis \}
\end{align*}}}

The equivalent trace with delays can be written as follows:

{\small\parbox{.3\textwidth}{\begin{align*}
&traces(G_9) \equiv \\
%\{ & \langle shift(x), delay(A,B), shift(y), reduce(C_1), switch(a), resolve(A_1), shift(\$), reduce(S_1) \rangle,\\
%   & \langle shift(x), delay(A,B), shift(y), reduce(C_1), switch(b), resolve(B_1), shift(\$), reduce(S_2) \rangle,\\
%   & \langle shift(x), delay(A,B), shift(y), reduce(C_1), switch(y), reduce(C_2), switch(a), resolve(A), shift(\$), \\&\qquad reduce(S_1) \rangle,\\
%   & \langle shift(x), delay(A,B), shift(y), reduce(C_1), switch(y), reduce(C_2), switch(b), resolve(B), shift(\$), \\&\qquad reduce(S_2) \rangle,\\
\{ & \langle shift(x), delay, shift(y), reduce(C_1), switch(a), resolve(A_1), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), delay, shift(y), reduce(C_1), switch(b), resolve(B_1), shift(\$), reduce(S_2) \rangle,\\
   & \langle shift(x), delay, shift(y), reduce(C_1), switch(y), reduce(C_2), switch(a), resolve(A), shift(\$), \\&\qquad reduce(S_1) \rangle,\\
   & \langle shift(x), delay, shift(y), reduce(C_1), switch(y), reduce(C_2), switch(b), resolve(B), shift(\$), \\&\qquad reduce(S_2) \rangle,\\
   & \textellipsis \}
\end{align*}}}

%S1 { A{x} C1{y} a }
%S2 { B{x} C1{y} b }
%S1 { A{x} C2{ C1{y} y } a }
%S2 { B{x} C2{ C1{y} y } b }


%%%%%%%%%%%%% Example 10: Looking ahead into a cycle
\subsubsection{Look ahead into a cycle}
\begin{tabular}[t]{cl}
\parbox{.3\textwidth}{
\begin{align*}
G_{10} \equiv \quad & A_1 \rightarrow x\\
                  & B_1 \rightarrow x\\
                  & C_1 \rightarrow a\\
                  & C_2 \rightarrow y C\\
                  & D_1 \rightarrow b\\
                  & D_2 \rightarrow y D\\
                  & S_1 \rightarrow A C \$\\
                  & S_2 \rightarrow B D \$
\end{align*}}
\parbox{.6\textwidth}{Sometimes the lookahead itself will be contained inside a cycle.
In this example the preceeding rules $A_1$ and $B_1$ depend on a lookahead within the recursive rules $C_2$ and $D_2$.}
\end{tabular}

{\small\parbox{.3\textwidth}{\begin{align*}
&traces(G_{10}) \equiv \\
\{ & \langle shift(x), reduce(A_1), switch(a), reduce(C_1), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), reduce(B_1), switch(b), reduce(D_1), shift(\$), reduce(S_2) \rangle,\\
   & \langle shift(x), reduce(A_1), switch(y), switch(a), reduce(C_1), reduce(C_2), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), reduce(B_1), switch(y), switch(b), reduce(D_1), reduce(D_2), shift(\$), reduce(S_2) \rangle,\\
   & \textellipsis \}
\end{align*}}}

The equivalent trace with delays can be written as follows:

{\small\parbox{.3\textwidth}{\begin{align*}
&traces(G_{10}) \equiv \\
% \{ & \langle shift(x), delay(A, B), switch(a), reduce(C_1), resolve(A), shift(\$), reduce(S_1) \rangle,\\
%    & \langle shift(x), delay(A, B), switch(b), reduce(D_1), resolve(B), shift(\$), reduce(S_2) \rangle,\\
%    & \langle shift(x), delay(A, B), switch(y), switch(a), reduce(C_1), reduce(C_2), resolve(A), shift(\$), \\&\qquad reduce(S_1) \rangle,\\
%    & \langle shift(x), delay(A, B), switch(y), switch(b), reduce(D_1), reduce(D_2), resolve(B), shift(\$), \\&\qquad reduce(S_2) \rangle,\\
 \{ & \langle shift(x), delay, switch(a), reduce(C_1), resolve(A), shift(\$), reduce(S_1) \rangle,\\
    & \langle shift(x), delay, switch(b), reduce(D_1), resolve(B), shift(\$), reduce(S_2) \rangle,\\
    & \langle shift(x), delay, switch(y), switch(a), reduce(C_1), reduce(C_2), resolve(A_1), shift(\$), reduce(S_1) \rangle,\\
    & \langle shift(x), delay, switch(y), switch(b), reduce(D_1), reduce(D_2), resolve(B_1), shift(\$), reduce(S_2) \rangle,\\
    & \textellipsis \}
\end{align*}}}

%S1 { A{x} C1{a} $ }
%S2 { B{x} D1{b} $ }
%S1 { A{x} C2{ y C1{a} } $ }
%S2 { B{x} D2{ y D1{b} } $ }


%%%%%%%%%%%%% Example 11: Looking ahead outside multiple nested cycles
\subsubsection{Look ahead outside multiple nested cycles}
\begin{tabular}[t]{cl}
\parbox{.3\textwidth}{
\begin{align*}
G_{11} \equiv \quad & A_1 \rightarrow x\\
                  & A_2 \rightarrow A C \\
                  & B_1 \rightarrow x\\
                  & B_2 \rightarrow B D\\
                  & C_1 \rightarrow y\\
                  & C_2 \rightarrow y C\\
                  & D_1 \rightarrow y\\
                  & D_2 \rightarrow y D\\
                  & S_1 \rightarrow A a \$\\
                  & S_2 \rightarrow B b \$
\end{align*}}
\parbox{.6\textwidth}{One pathological case to take into account is a grammar where multiple nested recursion takes place in a single rule yet both branches of tree require lookahead outside of the cycles. In this case notice that $A$ and $B$ are both 
cycles, yet within these also generate additional cycles using $C$ and $D$. Example strings include $xyyyyyyyyxyyyya$ and $xyyxyxyb$ corresponding to productions for $A$ and $B$ respectively.
For convenience we could write these strings using Kleene closures as $(x(y^*))^+a$ and $(x(y^*))^+b$ where $+$ indicates one or more repetitions and $*$ indicates zero or more repetitions.}
\end{tabular}

{\small\parbox{\textwidth}{\begin{align*}
&traces(G_{11}) \equiv \\
\{ & \langle shift(x), reduce(A_1), switch(a), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), reduce(B_1), switch(b), shift(\$), reduce(S_2) \rangle,\\
   & \langle shift(x), reduce(A_1), switch(y), reduce(C_1), reduce(A_2), switch(a), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), reduce(B_1), switch(y), reduce(D_1), reduce(B_2), switch(b), shift(\$), reduce(S_2) \rangle,\\
   & \langle shift(x), reduce(A_1), switch(y), reduce(C_1), reduce(A_2), switch(x), reduce(A_2), switch(a), \\ & \qquad shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), reduce(B_1), switch(y), reduce(D_1), reduce(B_2), switch(x), reduce(B_2), switch(b), \\ & \qquad shift(\$), reduce(S_2) \rangle,\\
   & \langle shift(x), reduce(A_1), switch(y), reduce(C_1), reduce(A_2), switch(x), reduce(A_1), switch(y), \\ & \qquad reduce(C_1), reduce(A_2), switch(a), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), reduce(B_1), switch(y), reduce(D_1), reduce(B_2), switch(x), reduce(B_1), switch(y), \\ & \qquad reduce(D_1), reduce(B_2), switch(b), shift(\$), reduce(S_2) \rangle,\\
   & \textellipsis \\
   & \langle shift(x), reduce(A_1), switch(y), reduce(C_1), switch(y), reduce(A_2), switch(x), reduce(A_1), \\ & \qquad switch(a), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), reduce(B_1), switch(y), reduce(D_1), switch(y), reduce(B_2), switch(x), reduce(B_1), \\ & \qquad switch(b), shift(\$), reduce(S_2) \rangle,\\
   & \langle shift(x), reduce(A_1), switch(y), reduce(C_1), switch(y), reduce(A_2), switch(x), reduce(A_1), \\ & \qquad switch(y), reduce(C_1), reduce(A_2), switch(a), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), reduce(B_1), switch(y), reduce(D_1), switch(y), reduce(B_2), switch(x), reduce(B_1), \\ & \qquad switch(y), reduce(D_1), reduce(B_2), switch(b), shift(\$), reduce(S_2) \rangle,\\
   & \textellipsis \}
\end{align*}}}

The equivalent trace with delays can be written as follows:

{\small\parbox{\textwidth}{\begin{align*}
&traces(G_{11}) \equiv \\
\{ & \langle shift(x), delay, switch(a), resolve(A_1), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), delay, switch(b), resolve(B_1), shift(\$), reduce(S_2) \rangle,\\
   & \langle shift(x), delay, switch(y), delay, delay, switch(a), resolve(A_2), resolve(C_1), resolve(A_1), \\ & \qquad shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), delay, switch(y), delay, delay, switch(b), resolve(B_2), resolve(D_1), resolve(B_1), \\ & \qquad shift(\$), reduce(S_2) \rangle,\\
   & \langle shift(x), delay, switch(y), delay, reduce(A_2), switch(x), delay, switch(a), resolve(A_2), \\ & \qquad resolve(C_1), resolve(A_1), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), delay, switch(y), delay, delay, switch(x), delay, switch(y), switch(b), resolve(D_1), \\ & \qquad resolve(B_2), resolve(D_1), resolve(B_1), shift(\$), reduce(S_2) \rangle,\\
   & \langle shift(x), delay, switch(y), delay, reduce(A_2), switch(x), delay, switch(y), delay, reduce(A_2), \\ & \qquad switch(a), resolve(C_1), resolve(A_1), resolve(C_1), resolve(A_1), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), delay, switch(y), delay, reduce(B_2), switch(x), delay, switch(y), delay, reduce(B_2), \\ & \qquad switch(b), resolve(D_1), resolve(B_1), resolve(D_1), resolve(B_1), shift(\$), reduce(S_2) \rangle,\\
   & \textellipsis \\
   & \langle shift(x), delay, switch(y), delay, switch(y), reduce(A_2), switch(x), reduce(A_1), switch(a), \\ & \qquad resolve(C_1), resolve(A_1), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), delay, switch(y), delay, switch(y), reduce(B_2), switch(x), reduce(B_1), switch(b), \\ & \qquad resolve(D_1), resolve(B_1), shift(\$), reduce(S_2) \rangle,\\
   & \langle shift(x), delay, switch(y), delay, switch(y), reduce(A_2), switch(x), reduce(A_1), switch(y), \\ & \qquad reduce(C_1), reduce(A_2), switch(a), resolve(C_1), resolve(A_1), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), delay, switch(y), delay, switch(y), reduce(B_2), switch(x), reduce(B_1), switch(y), \\ & \qquad reduce(D_1), reduce(B_2), switch(b), resolve(D_1), resolve(B_1), shift(\$), reduce(S_2) \rangle,\\
   & \textellipsis \}
\end{align*}}}

This particular trace has special significance to our work as it is a counter-example that cannot be generated given our existing instruction set.
In the conclusion of this paper we will look at a possible approach to resolving this problem in future work.

Reasoning about the traces of these grammars becomes increasingly difficult as cycles are specified in various ways.
A formal method for describing traces and determining whether their equivalent traces with delays can be produced by an LD parser is needed.

\subsection{A left-to-right parser with delayed reductions}
We've seen that the instruction set given could in theory be used to construct parsers for a large range of context-free grammars if we can show that such a parser will produce the traces given above.
As discussed in the previous section, an LD parser recognizes production rules left-to-right, bottom-up manner. 
Assuming that no shuffling of instructions take place, this type of parser is trivially equivalent to an \emph{LR(0)} parser.

However we wish to construct parsers for any $k$ including $unbounded k$. In \cite{knuth65} Knuth showed that determining whether a grammar is in the class $LR(k)$ for some unspecified $k$ is undecidable in general. 
This makes us wary of attempting to solve problems that are intractable by nature. 
Hence, let us first attempt to establish what is theoretically possible.

For some class of grammars it will be necessary to show that a finite parser\footnote{Note that we refer to the parser here in terms of the generated bitcode stream.} can be constructed that generates an equivalent trace to the mechanically generated trace.
Generating a simple shift-reduce trace for a grammar is similar to enumerating all strings that the grammar can generate.

Another view of this idea is that if a parser can generate the same sequences of strings that a grammar can, then it is possible to \emph{recognize} the grammar with the given parser.
However, if a parser can generate the same sequences of traces, then it is also capable of generating the same structure that the grammar does.
In other words we will say that the parser is capable of \emph{translating} to a grammar if it can generate an equivalent trace.

Both strings and traces can have infinite nested `substrings' or `subtraces' inside their main traces.
By using a naive algorithm, the generator will get `stuck' inside a subtrace and never explore the remainder of the grammar.
An procedure for expanding the traces of the grammar in a `breadth-first' manner such that all branches of the grammar can make progress is needed.
In other words, is there a function $E(G, n)$ producing a set of traces of the grammar $G$ with $n \in \mathbb{N}$ such that the following relations hold?
\begin{equation} E(G, n) \sqsubseteq E(G, n+1) \sqsubseteq \top_{G} \end{equation} and \begin{equation} lim_{n \to \infty} E(G,n) = \top_{G} \end{equation}

Here $\sqsubseteq$ is defined as a total ordering on sets of traces such that $A \sqsubset B \quad \Longleftrightarrow \quad A \subset B$ and $B$ expands the traces as demonstrated bellow.
In the total ordering two special elements are used namely, $\bot_{G} \equiv \{ \langle \rangle \}$, the empty trace, and $\top_{G} \equiv traces(G)$, the---possibly infinite---set of all potential traces of $G$.

Consider the following grammar:
\begin{align*}
G \equiv \quad & A_1 \rightarrow x\\
               & B_1 \rightarrow x\\
               & C_1 \rightarrow a\\
               & C_2 \rightarrow y C\\
               & D_1 \rightarrow b\\
               & D_2 \rightarrow y D\\
               & S_1 \rightarrow A C \$\\
               & S_2 \rightarrow B D \$
\end{align*}

The set of traces can be expanded as follows:\\
First add the empty trace to the set. 
Take all the starting rules, in this case, $S_1$ and $S_2$.
Next take all nonterminals on the right-hand side and expand them once using all possible rules. 

If some rules are incomplete, then continue in the next iteration. This guarantees that the entire grammar can be enumerated in a single loop such that $lim_{n \to \infty} ET(G,n) = \top_{G}$.
$IT(G, n)$ will represent all `incomplete' traces that need to go through more rounds of expansion before being added the sets generated by $ET(G, n)$.
If we define a function $ExpandRound$ that does just this then it may be convenient to write the equation $ET(G, n+1) \equiv ET(G, n) \cup ExpandRound(IT(G, n))$.

{\small\begin{align*}
ET(G, 0) & = \{ \langle \rangle \}\\
IT(G, 0) & = \{\\
\quad & \langle \mathbf{Expand(S_1)} \rangle,\\
\quad & \langle \mathbf{Expand(S_2)} \rangle \}\\
ET(G, 1) & = \{ \langle \rangle \}\\
IT(G, 1) & = \{ \\
\quad & \langle \mathbf{Expand(A_1)}, \mathbf{Expand(C_1)}, shift(\$), reduce(S_1) \rangle,\\
\quad & \langle \mathbf{Expand(A_1)}, \mathbf{Expand(C_2)}, shift(\$), reduce(S_1) \rangle,\\
\quad & \langle \mathbf{Expand(B_1)}, \mathbf{Expand(D_1)}, shift(\$), reduce(S_2) \rangle, \\
\quad & \langle \mathbf{Expand(B_1)}, \mathbf{Expand(D_2)}, shift(\$), reduce(S_2) \rangle \\
\quad & \}\\
ET(G, 2) & = \{\\
\quad & <>, \\
\quad & <shift(x), reduce(A_1), shift(a), reduce(C_1), shift(\$), reduce(S_1)>,\\
\quad & <shift(x), reduce(B_1), shift(b), reduce(D_1), shift(\$), reduce(S_1)>\\
\}\\
IT(G, 2) & = \{ \\
\quad & <shift(x), reduce(A_1), shift(y), \mathbf{Expand(C_1)}, reduce(C_2), shift(\$), reduce(S_1)>,\\
\quad & <shift(x), reduce(A_1), shift(y), \mathbf{Expand(C_2)}, reduce(C_2), shift(\$), reduce(S_1)>,\\
\quad & <shift(x), reduce(B_1), shift(y), \mathbf{Expand(D_1)}, reduce(D_2), shift(\$), reduce(S_1)>,\\
\quad & <shift(x), reduce(B_1), shift(y), \mathbf{Expand(D_2)}, reduce(D_2), shift(\$), reduce(S_1)>\\
 \}\\
 \textellipsis
\end{align*}}


%As discussed in the previous section, an LD parser recognizes production rules left-to-right, bottom-up manner. 
%Assuming that no shuffling of instructions take place, this type of parser is equivalent to an \emph{LR(0)} parser.
%However, we have also given the parser the ability to resolve delayed rules in a right-to-left, top-down manner.

%\emph{ (TODO: Discuss how this affects our work) }\\


%\emph{TODO:
%When calling procedures in a recursive-ascent parser we are intuitively traversing the grammar's state diagram in a bottom-up manner.
%However, when returning from a function to a previous state we revert to a top-down traversal.
%Hence traversing the call stack in an upward or a downward direction can be viewed as switching between top-down versus bottom-up recognition of the grammar. 
%In more general terms, when the bottom-up technique cannot reduce a rule it becomes resolved in a top-down fashion later on by replacing the delayed placeholder token.}

%%%%%%%%%%%%%%%%%%% IMPLEMENTATION
\section{Implementation}
\subsection{Parser driver}

In our implementation a generated parser is executed by an interpreter for the instruction set that we developed in the previous section of the paper.
The basic interpreter is described here with most of the optimizations and certain technical details removed for the sake of clarity. 
Additional implementation details will be briefly described in the last part of this section along with the internal representation of the generated parsers.\\

Three procedures are responsible for the execution of the parser. 
The first is a call to the lexer which is responsible for tokenizing the text file into the correct format for our main parsing functions.
We will gloss over this step as it is outside the scope of this work, but any suitable lexer can be dropped in and used at this point.
Next the recognition pass is executed, which takes the stream of lexical tokens and outputs a stream of rules. 
The translation pass is then responsible for iterating through these rules in order to assemble a concrete syntax tree, the resulting output of the parsing operation.
\emph{TODO: what about syntax errors?}

\begin{figure}[!ht]
\begin{center}
\begin{gcl}
\PROC ParseText(parseTree, textStream)
LexicalAnalysis(lexStream, textStream);
RecognitionPass(rulesStream, lexStream);
TranslationPass(parseTree, rulesStream, lexStream, textStream);
\CORP
\end{gcl}
\caption{Three main stages of the drivers.}
\end{center}
\end{figure}

The primary operations performed by the recognition pass involve decoding instructions from the generated parser and their application on the lex stream and the rules stream. 
Note that the generated parser instructions is encoded in a bitcode array whose internal format is described more fully later on.
Sizes of records in the bitcode stream may vary, but for presentation purposes we will assume that every record in the stream can be accessed using a simple subscript.
In reality iteration over the encoded instructions occurs in a more ad hoc manner.

\clearpage
\begin{figure}[!ht]
\begin{center}
\begin{gcl}
\PROC RecognitionPass(rulesStream, lexStream, bitcodeStream)
    i \becomes 0;
    \DO lexStream_i \neq TOKEN\_\$ \rightarrow
      \DO DecodeShiftOp(bitcodeStream_j, lexStream_i) \rightarrow
      \quad Append(rulesStream, token);
      \BAR DecodeReduceOp(bitcodeStream_j) \rightarrow
      j \becomes j + 1;
      \BAR DecodeResolveOp(bitcodeStream_j) \rightarrow
      j \becomes j + 1;
      \BAR DecodeDealayOp(bitcodeStream_j) \rightarrow
      j \becomes j + 1;
      \BAR DecodeReturnOp(bitcodeStream_j) \rightarrow
      j \becomes j + 1;
      \BAR DecodeAcceptOp(bitcodeStream_j) \rightarrow
      j \becomes j + 1;
    \OD
    i \becomes i + 1;
  \OD
\CORP
\end{gcl}
\caption{Basic implementation .}
\end{center}
\end{figure}

Once the recognition pass has filled the rules stream, the translation pass can take over. 
In much the same manner as any LR parser, the translation pass can simply iterate over the rules assembling the parse tree in a bottom up fashion. 
Because every production rule is a concrete instance of a grammar production, a parse tree can be constructed deterministically without reference to the lex stream 
other than to copy lexical tokens or their underlying data to the new parse tree structure.
Alternatively one could also construct this tree in a top-down fashion by simply iterating through the rules in reverse.
\begin{figure}[!ht]
\begin{center}
\begin{gcl}
\PROC TranslationPass(parseTree, rulesStream, lexStream, textStream)
treeNode \becomes parseTree;
Append(parseTree, rulesStream_{j});
j \becomes j + 1;
\CORP
\end{gcl}
\caption{An interpreter for the LD parsing DSL.}
\end{center}
\end{figure}

\subsection{Internal representation}
We will describe our parsers' internal representation as an encoded stream of opcodes accompanied by their arguments. 
We will refer to this stream as the \emph{bitcode}\footnote{This is perhaps more commonly refered to as \emph{bytecode}, refering to the idea that an opcode is encoded by a single byte. We've chosen to use the more general term \emph{bitcode} employed by the popular LLVM infrastructure.} encoding of a parser.
In our implementation the bitcodes for a single instruction is packed into a varying number of bytes.

An implementation may however choose any suitable internal representation and we anticipate that implementors may choose to translate the bitcode to the internal representation suitable for their target language or machine.
\texttt{opcode, [arguments...]}

We've made certain optimistic optimizations in order to minimize the cost of instruction decoding. 
For this reason an opcode is often a packed representation of the particular instruction.
In particular terminal tokens always represent $shift$ instructions while nonterminal tokens represent $reduce$ instructions.
This is done by using the highest bit to distinguish between terminals and nonterminals and hence also between $shift$ and $reduce$ actions.
An additional bit flags in highest byte is then used to modify the  $reduce$ instructions by changing it into the corresponding $resolve$ instruction.
Finally, certain opcode values are reserved for the remaining instructions which also include additional arguments.

\begin{itemize}
\item A special token called $ignore$ represents the $delay$ instruction and is also used as the placeholder token that is appended to the rules buffer by the $delay$ action.
\item The $switch$ opcode corresponds to the instruction by the same name. It is coded as the string\\\\
      \texttt{switch, length, (terminal, target\_state)}$^\texttt{*}$\\\\
      where $length$ is the number of pairs that map terminal tokens to their target states.
\item $return$ is the simple opcode corresponding to the same instruction but takes no arguments
\item $goto$ is the opcode corresponding to the instruction coded as\\\\
      \texttt{goto, source\_state, target\_state}\\
\item $accept$ is the opcode corresponding to the final $return$ instruction that ends the recognition pass.
\end{itemize}

This configuration makes decoding $shift$ instructions efficient and convenient since we can simply test for equality with lexical tokens read from the input stream. 
The next opcodes that are decoded are the $reduce$ and $resolve$ instructions, followed by tests for each of $switch$, $return$, $goto$ and finally $accept$.

\subsection{Parser construction algorithm}
Constructing a parser by hand is a time consuming and error prone process. For this reason an automatic construction algorithm is desireable.
As a prototype we have constructed a parser generator that is sufficient for a limited subset of unambiguous grammars.
While the possible grammars cannot yet be classified, many grammars with large lookahead requirements (including cycles) will work.

Given a grammar specification, the construction algorithm outputs the bitcode representation to be used with the interpreter.

We will use a state graph to represent various possible states that the parser can inhabit. 
Every \emph{state} is composed of a set of unique items. 
In turn an \emph{item} refers to specific production rule and a state cursor.
Finally states have two types of edges leading from one state to another. 
The first type represents $switch$ branches and the second represents $goto$ edges. We also distinguish between \emph{leading} and \emph{following} edges with reference to a particular state node.\\

Along with a grammar specification, a user must also specify the top-most nonterminal in the grammar which, given a conforming input stream, should form the root node of the final parse tree.\\

For example, using the following grammar:

\parbox{.3\textwidth}{\begin{align*}
G \equiv \quad & A_1 \rightarrow x\\
               & B_1 \rightarrow x\\
               & B_2 \rightarrow x z\\
               & C_1 \rightarrow y\\
               & C_2 \rightarrow C y\\
               & S_1 \rightarrow A C a \$\\
               & S_2 \rightarrow B C b \$
\end{align*}}

The initial state will contain the rules corresponding the root production of the grammar, in this case $S_1$ and $S_2$.\\
\clearpage
\begin{figure}[!ht]
\begin{center}
\begin{tikzpicture}
   \path (0,0) node[draw,shape=rectangle] (S0) 
   {
     \parbox{0\textwidth}{\begin{align*}
     & S_1 \rightarrow . A C a \$\\
     & S_2 \rightarrow . B C b \$
     \end{align*}}
   };
   \path (-1.5,0.9) node (S0n) {0};
\end{tikzpicture}
\caption{An example root state node in the parser generator.}
\end{center}
\end{figure}

The period on the right-hand side of each rule is the cursor indicating the progress of the rule in a particular parser state.
For example, given an item $W_1 \rightarrow x . y$, this would indicate that the terminal $x$ has been recognized, and only $y$ remains to be parsed in order to recognize this rule.\\

Once an initial state node has been constructed the main loop of the algorithm takes over. 
The state is expanded until a non-deterministic \emph{branch state} is reached where a parser would be required to make a choice corresponding to the $switch$ action.
During the expansion process parsing instructions are recorded into an \emph{instruction table} which will be encoded into bitcode stream once the algorithm completes.
The instruction table is an array of dynamically growing `strings' of instructions. 
Each \emph{instruction string} corresponds to a particular branch state in the state graph.
An alternative view of this idea is that every instruction string could represent a procedure of a recursive ascent parser written in an imperative programming language.
The following steps iteratively expand the state until a branch state is reached.

\begin{enumerate}
\item Expand items --- The token directly to the right of every cursor is inspected. 
If the token is a nonterminal token, then the item is replaced with the following items: For every rule producing this nonterminal on the left, duplicate the item and annotate it with this rule number.
In addition a new item corresponding with the given rule and a cursor in the initial position may be added to the state whenever an old item is annotated. 
An exception is made only when such an item already exists in the state, in which case only the annotation takes place.
Once this process is completed the newly added (unannotated) items are again expanded until no more unnanoted items with nonterminals following the cursor can be found. 
For example, following the expansion procedure our example state will look like this.

\begin{figure}[!ht]
\begin{center}
\begin{tikzpicture}
   \path (0,0) node[draw,shape=rectangle] (S) 
   {
     \parbox{0\textwidth}{\begin{align*}
     & S_1 \rightarrow . A C a \$ \quad {(A_1)}\\
     & S_2 \rightarrow . B C b \$ \quad {(B_1)}\\
     & S_2 \rightarrow . B C b \$ \quad {(B_2)}\\
     & A_1 \rightarrow . x\\
     & B_1 \rightarrow . x\\
     & B_2 \rightarrow . x z
     \end{align*}}
   };
   \path (-2,2) node (S0n) {0};
\end{tikzpicture}
\caption{An example state after items have been expanded.}
\end{center}
\end{figure}

Notice that there are two instances of the rule $S_2 \rightarrow . B C b \$$ annotated with the rules $(B_1)$ and $(B_2)$.

\item Shift terminal tokens --- Once all items have been expanded, each item is once again inspected. 
If a terminal token follows the cursor, it is moved one token forward and the terminal token is placed into a temporary set to be recognized.
For historical reasons, the set is known
as the \emph{FIRST} set\footnote{$LR(k)$ parsers make a distinction between FIRST set and FOLLOW sets indicating, respectively, 
the first terminal tokens that can be recognized in a string of grammar tokens and the sets of terminals that could follow each of the FIRST terminals.} of the current state.
Because there must be at least one terminal token in the grammar (specifically the end-of-stream token \$) there is at least one token in the FIRST set.
If only one unique terminal exists in the set, then the state is \emph{deterministic} and hence a $shift$ instruction will be generated and appended to the current string of instructions in the instruction table.
However, if multiple unique terminals exist in the FIRST set of the state, then the state is a non-deterministic branch state and a $switch$ instruction is generated instead.
\begin{itemize}
\item Given that a $shift$ instruction must be generated, the tool checks whether all items in the state are $complete$ in the sense described below. 
If they are not then the entire procedure is repeated until a set of FIRST terminals must once again be shifted. 
Otherwise, the tool returns to the state that preceded this one.
\item Given that a $switch$ instruction must be generated, the current state must be duplicated once for every unique terminal in the switch. 
An edge labeled with the terminal token connects every new state to its source.
However, every duplicate state must be filtered so that only the rules partaking in a particular branch of the switch are included.
All other items are removed from the new state.
\end{itemize}
An item is \emph{completed} once the cursor has moved past all remaining tokens on the right-hand side of the rule.
Once either a $shift$ or a $switch$ has been generated, several items in the state may be completed. 
A special subroutine is responsible for handling these completed items which in turn generates $reduce$ and $delay$ instructions depending on the context.
In the case of a $shift$ action, the items in the original state is completed whereas, in the case of a $switch$, each of the new states are processed by the routine.\\

\item[$\bullet$] Complete items --- Every time a FIRST set is processed, the generator must test whether all items in the state are complete. 
In the case of a $shift$ action, the current state is tested and if all items are complete, a $return$ instruction must be generated. In 
\end{enumerate}

For example, iterative application of steps 1 and 2 will result in the following state changes

\begin{figure}[!ht]
\begin{center}
\parbox{.3\textwidth}{\begin{tikzpicture}
   \path (0,0) node[draw,shape=rectangle] (S) 
   {
     \parbox{0\textwidth}{\begin{align*}
     & S_1 \rightarrow . A C a \$ \quad {(A_1)}\\
     & S_2 \rightarrow . B C b \$ \quad {(B_1)}\\
     & S_2 \rightarrow . B C b \$ \quad {(B_2)}\\
     & A_1 \rightarrow x .\\
     & B_1 \rightarrow x .\\
     & B_2 \rightarrow x . z
     \end{align*}}
   };
   \path (-2,2) node (S0n) {0};
\end{tikzpicture}}
\parbox{.65\textwidth}{Parsing instructions:\\$shift(x)$}
\caption{The $shift$ instruction is deduced by stepping over the $x$ terminal.}
\end{center}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\parbox{.3\textwidth}{\begin{tikzpicture}
   \path (0,0) node[draw,shape=rectangle] (S) 
   {
     \parbox{0\textwidth}{\begin{align*}
     & S_1 \rightarrow A . C a \$ \quad {(A_1)}\\
     & S_2 \rightarrow B . C b \$ \quad {(B_1)}\\
     & S_2 \rightarrow . B C b \$ \quad {(B_2)}\\
     & B_2 \rightarrow x . z
     \end{align*}}
   };
   \path (-2,1.5) node (S0n) {0};
\end{tikzpicture}}
\parbox{.65\textwidth}{Parsing instructions:\\$shift(x), delay(A_1, B_1)$}
\caption{A $delay$ instruction is deduced by completing rules $A_1$ and $B_1$. Take note of rule $B_2$ which cannot be completed due to the remaining terminal $z$.}
\end{center}
\end{figure}

\clearpage

\begin{figure}[!ht]
\begin{center}
\parbox{.3\textwidth}{\begin{tikzpicture}
   \path (0,0) node[draw,shape=rectangle] (S)
   {
     \parbox{0\textwidth}{\begin{align*}
     & S_1 \rightarrow A . C a \$ \quad\\
     & S_2 \rightarrow B . C b \$ \quad\\
     & S_2 \rightarrow . B C b \$ \quad {(B_2)}\\
     & B_2 \rightarrow x . z\\	
     & C_1 \rightarrow . y\\
     & C_2 \rightarrow . C y
     \end{align*}}
   };
   \path (-2,2) node (S0n) {0};
\end{tikzpicture}}
\parbox{.65\textwidth}{Parsing instructions:\\$shift(x), delay(A_1, B_1), switch(y \mapsto 1, z \mapsto 2)$}
\caption{A $switch$ instruction is deduced by stepping over terminals $y$ and $z$.}
\end{center}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\parbox{.65\textwidth}{\begin{tikzpicture}
   \path (0,0) node[draw,shape=rectangle] (S0) 
   {
     \parbox{0\textwidth}{\begin{align*}
     & S_1 \rightarrow A . C a \$\\
     & S_2 \rightarrow B . C b \$\\
     & S_2 \rightarrow . B C b \$ \quad {(B_2)}\\
     & B_2 \rightarrow x . z\\	
     & C_1 \rightarrow . y\\
     & C_2 \rightarrow . C y
     \end{align*}}
   };
   \path (-2,2) node (S0n) {0};

   \path (5,2.5) node[draw,shape=rectangle] (S1) 
   {
     \parbox{0\textwidth}{\begin{align*}
     & S_1 \rightarrow A . C a \$\\
     & S_2 \rightarrow B . C b \$\\
     & C_1 \rightarrow . y\\
     & C_2 \rightarrow . C y
     \end{align*}}
   };
   \path (3.5,4) node (S1n) {1};

   \path (5.3,-2) node[draw,shape=rectangle] (S2) 
   {
     \parbox{0\textwidth}{\begin{align*}
     & S_2 \rightarrow . B C b \$ \quad {(B_2)}\\
     & B_2 \rightarrow x . z
     \end{align*}}
   };
   \path (3.3,-1.1) node (S2n) {2};

   %todo: edge lables and arrows
   \draw (S0) -- (S1) (S0) -- (S2);

\end{tikzpicture}}
\caption{Generating new states for the switch instruction.}
\end{center}
\end{figure}

\clearpage

\begin{figure}[!ht]
\begin{center}
\begin{tikzpicture}
   \path (0,0) node[draw,shape=rectangle] (S1) 
   {
     \parbox{0\textwidth}{\begin{align*}
     & S_1 \rightarrow A . C a \$ \quad {(C_1)}\\
     & S_1 \rightarrow A . C a \$ \quad {(C_2)}\\
     & S_2 \rightarrow B . C b \$ \quad {(C_1)}\\
     & S_2 \rightarrow B . C b \$ \quad {(C_2)}\\
     & C_1 \rightarrow . y\\
     & C_2 \rightarrow . C y \quad {(C_1)}\\
     & C_2 \rightarrow . C y \quad {(C_2)}
     \end{align*}}
   };
   \path (-2,2.3) node (S1n) {1};
\end{tikzpicture}
\caption{Expanding state 1}
\end{center}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\parbox{.3\textwidth}{\begin{tikzpicture}
   \path (0,0) node[draw,shape=rectangle] (S2) 
   {
     \parbox{0\textwidth}{\begin{align*}
     & S_2 \rightarrow B . C b \$ \quad {(B_2)}\\
     & B_2 \rightarrow x z .
     \end{align*}}
   };
   \path (-2,0.9) node (S2n) {2};
\end{tikzpicture}}
\parbox{.65\textwidth}{Parsing instructions:\\$shift(z)$}
\caption{A $shift$ instruction is deduced by stepping over the $z$ terminal.}
\end{center}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\parbox{.3\textwidth}{\begin{tikzpicture}
   \path (0,0) node[draw,shape=rectangle] (S2) 
   {
     \parbox{0\textwidth}{\begin{align*}
     & S_2 \rightarrow B . C b \$ \quad {(C_1)}\\
     & S_2 \rightarrow B . C b \$ \quad {(C_2)}\\
     & C_1 \rightarrow . y\\
     & C_2 \rightarrow . C y \quad {(C_1)}\\
     & C_2 \rightarrow . C y \quad {(C_2)}
     \end{align*}}
   };
   \path (-2,0.9) node (S2n) {2};
\end{tikzpicture}}
\caption{Expanding state 2.}
\end{center}
\end{figure}

At this point our instruction string will contain a shift, delay and a switch instructions.
\begin{enumerate}
\item[0] $shift(x), delay, switch(y \mapsto 1, z \mapsto 2)$\\
\item $shift(y), reduce(C_1), switch(a \mapsto ?, b \mapsto ?, y \mapsto ?)$\\
\item $shift(z), reduce(B_2), shift(y), reduce(C_1), switch(b \mapsto ?, y \mapsto ?)$
\end{enumerate}

%In the final section of this paper we will discuss the limitations of this algorithm and discuss some proposals for extension that take into account the full set of context free grammars. 
%(TODO: We will also show that parsers built using this technique already includes the entire class of $LR(1)$ grammars)

\section*{Results}

It is tempting to classify an LD parser performance according to the number of operations per input token.
However, the class of the grammar that the parser represents should also be taken into account for comparison with other parsers.
For simplicity only the recognition pass is considered here. 
The translation pass builds a parse tree in much the same way as any $LR(0)$ parser, therefore the expected worstcase is $O(n)$.

Analyzing the performance of each instruction individually is relatively straightforward. 
For this reason it is easiest to characterize the performance of LD parsers according to the number of parsing instructions an LD parser may take.
The table bellow lists the performance of each individual operation according to its arguments.

\begin{table}[!ht]\begin{center}
  \begin{tabular}{|l|l|}
  \hline
  \textbf{Operation} & \textbf{Time} \\
  \hline
  $shift$            & constant \\ \hline
  $switch$           & linear in the number of arguments \\ \hline
  $accept$/$return$  & constant \\ \hline
  $goto$             & linear in the number of arguments \\ \hline
  $delay$            & constant \\ \hline
  $reduce$           & constant \\ \hline
  $resolve$          & constant \\
  \hline
  \end{tabular}
\caption{Performance of individual parsing operations.}
\end{center}\end{table}

It is possible for the operations that exhibit linear time performance, $shift$ and $goto$, to encode their arguments in a flattened binary tree or perform similar tricks in order to decrease asymptotic access time.
However, the overhead of the additional operations and memory access is likely to dominate performance in practice since the number of arguments are expected to be few.
Since the original LR algorithms used a large lookup table to determine every action, it appears to be common practice to simply ignore the time for these simple recognition operations.

Given an input stream of $n$ tokens, every token in the input stream may only be tested once during the recognition pass.
Hence, the number of $shift$ and $switch$ operations executed by the parse will certainly be linear in $n$.

The number of reductions that take place seem intuitively related to both the number input tokens and the complexity of the given grammar specification.
However, since $reduce$ instructions do not take any lookahead into account $reduce$ instructions should have the same worst-case asymptotic performance regardless of the class of grammar used.
The performance can be related to $n$, the number of input tokens, and $j$, the maximum depth of the final parse tree.

Consider cases in which the grammar falls withing the class $LR(k)$ for some fixed $k$.
In the worst case the number of $reduce$ operations will be $(j - 1) \cdot n + 1$ since every token will be involved in at most $j$ productions, one of which has to be the root node of the parse tree.
When the grammar is unambiguous the number of rules with only nonterminals on the right-hand side will necessarily be constant.
Since every rule that is not one of these must consume at least one terminal, most analyses simply reduce $j$ to a constant term.
Therefore, for all unambiguous grammars the number of $reduce$ operations is roughly linear with respect to the number of input tokens.

%From a practical point of view we expect that sane grammars will not include many redundant \emph{alternative wording?} productions, especially inside recursive rules.
%Based on this assumption, it is our estimation that a typical average case performance of $o(n)$ is far more likely for most applications. (An argument that could most likely be applied to GLR and Earley parsers as well).

The pair of actions $delay$ and $resolve$ should exhibit similar performance to $reduce$.
We have already seen that the equivalent trace using delays may split a $reduce$ action into a single pair of $delay$ and $resolve$ actions.
The only difference here is that some delay actions may be resolved using an $ignore$ which does not correspond to any reduce action.
An $ignore$ token may be generated through a $resolve$ whenever two or more rules are evaluated simultaneously with lookahead. 
Using the same argumentation showing that $reduce$ is $O(n)$ with respect to the number of input tokens it can be seen that the number of possible reductions in any one of these rules is linear.
Assuming that the number of these simultaneous rules will be constant\footnote{An assertion that may require further investigation.}, the number of $ignore$ tokens will also be linear.

Finally, determining the upper limit for $return$ and $goto$ operations is trivial once one realizes that both operations must correspond to a preceding $switch$ operation. 
It follows that $return$ and $goto$ are both linear in $n$.\\

All of the above arguments seem to indicate that LD parsers can be constructed that will perform in linear---or at least near-linear---time for those unambiguous grammars that can be parsed using the instruction set.
Our initial preformance analysis appears favourable in comparison to the most efficient generalized parsing algorithms.

\section*{Conclusions}

In the course of this work we have designed and implemented a new parsing framework with a novel lookahead mechanism.
Folowing a great deal of refinement, our methodology has been distilled into a simple, easily understood set of instructions.
Modeling the parser as a domain specific language has aforded us the opportunity to apply well known existing semantic models to parsers.
Although a little unexpected at first, we came to the conclusion that concepts from programming language theory may be ideal for studying parsing theory.
This is not so suprising in retrospect given that grammars have a lot in common with programs and demonstrate similar decidability issues.

The distinction made between language recognition and language translation allowed us to separate these two concerns into separate passes, 
thereby simplifying lookahead greatly. %\emph{TODO: Another contribution is the recognition of the longest non-deterministic strings ...}

From an implementation point of view, a parser driver was implemented consisting of a lexer, parser interpreter and a tree builder.
In addition to this a prototype parser generator tool was implemented using our current work-in-progress construction algorithm.
Parsing instructions are encoded into a small bitcode format by the parser generator and passed to the interpreter by the parser driver.
Every parsing operation in the interpreter is small and simple and all auxiliary data structures are similarly uncomplicated. 
Therefore it is our hope and expectation that the interpreter will perform well in comparison to other automatically generated parsers, especially where generalized parsing is concerned.

\section*{Future work}

We consider our work thus far to be exploratory in nature.
As such, there remains a large amount of theoretical and practical work to be done.
Our main concern is increasing the generality of LD parsing in order to handle a larger class of context-free grammars, possibly including all ambiguous grammars. 
It should be noted that it is possible to annotate $delay$ operations with the possible reductions that could be performed at this step.
We see a possibility for future work in generating new rule numbers for various combinations of delayed rules, using these in place of the usual $ignore$ placeholder token.

%Another interesting problem related to parsing that we feel may be related to our work is the translation from one grammar to another.
%If such a translator could be automatically generated by encoding it in some kind of grammar then a powerful lookahead mechanism such as the one presented here could be used.
%While this idea is pure conjecture, perhaps such a translator could be used to automatically generate complex transformations inside optimizing compilers.

It has been demonstrated that the LD instruction set is capable of handling a wide variety of unambiguous context-free grammars but will not accept the special case where lookahead is required outside multiple nested recursions.
We put forward the idea that the instruction set might be extended, possibly by adding an alternative to the $return$ instruction that does not remove the last address from the callstack.

Knuth showed that determining whether a context-free grammar is ambiguous is undecidable in general\cite{knuth65}. 
This implies that certain concequences must be taken into account in our work. 
First off, determining whether a grammar is a suitable candidate for parsing using our current instruction set seems to be undecidable.
Although we have not yet experienced this problem we suspect that the issue may manifest as infinite growth in the state graph during the construction algorithm if certain grammars are used.
If this effect does occur it can simply be circumvented by specifying a limiting depth for the generator to use. 
Even so, this possibility warrants further study.

In the Results section a rough analysis of the performance of the LD parser was given.
The simplicity of our approach emboldens us to profile the efficiency of generated parsers.
Benchmarks should be set up in order to test the performance of generated parsers in comparison to other parser generators.
While asymptotic performance gives a useful metric of the scalability of parsing algorithms in various scenarios, the constant term tends to have a large effect on parsing performance in practice.
Furthermore, complex effects such as memory latency and branch misprediction may be missed by over-simplified mathematical models.
Many opportunities for optimization is still available especially where construction of the parsers are concerned.
Another possibility that could provide additional speedup is direct conversion of parsers to machine code using an optimizing compiler.
%Much of the research into parsing algorithms occured in the 70's. 
%Today's hardware has different performance characteristics which is still in a phase of rapid change. 
%In order to take advantage of modern hardware, now and in the future, we may need to re-evaluate the classical algorithms using different performance metrics.
%In order to take advantage of modern hardware, now and in the future, we may need to re-evaluate the classical algorithms using different performance metrics.

Finally, there are additional engineering concerns that also need to be resolved before LD parsing can be considered for industrial use.
In particular we have omitted error recovery, semantic actions, precedence rules and incremental parsing from our work thus far.

%\emph{ %BUSY HERE
%(TODO)\\
%Open problems: problematic unambiguous context-free grammars (nested lookahead problem + construction problem), progressive parsing (combining the recognition and builder passes), formal proofs of correctness\\
%Suggest that delays should be encoded as stacks of continuations similar to functional programming rather than as a simple callstack?\\
%Performance analysis and benchmarks\\
%Termination analysis\\
%Proof is needed for the absence of exponential growth of states in the generated parser\\
%Parallelization / pipelined implementations?\\
%Application to other grammar classes (E.g. ambiguous context-free and context-sensitive grammars)\\
%}

\section*{Acknowledgements}
I would like to thank Prof. Derrick Kourie and Prof. Bruce Watson of the University of Pretoria for their insightful suggestions and patient support during the development of this work.

\bibliographystyle{plain}
\bibliography{delayedcase}

\end{document}

