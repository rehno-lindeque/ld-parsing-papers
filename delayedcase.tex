
\documentclass[a4paper,11pt]{article}

%%%%%%%%%%%%%%%%%%% PACKAGES %%%%%%%%%%%%%%%%%%%
\usepackage{tikz}
\usepackage{geometry}
\geometry{a4paper}
\usepackage{gcl}
%\usepackage{pfs}
%\usepackage{algorithmic}
%\usepackage{graphicx}
%\usepackage{epstopdf}
%\usepackage{natbib}
%\usepackage{fancyvrb}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{proof}

%\usepackage{stmaryrd}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}

%%%%%%%%%%%%%%%%%%%% MACROS %%%%%%%%%%%%%%%%%%%%
% Grammars
%\newcommand {\GRAMMAR}
%  {\begin{indent} \begin{align*}}
%  %{\setlength{\parindent}{\GRAMMAR}\\
%  %\begin{indent}}

%\newcommand {\ENDGRAMMAR}
%  {\end{align*} \end{indent}}

\newenvironment{grammar}{\begin{align*}}{\end{align*}}
  
%  {\begin{indent} \begin{align*}}
%  {\end{align*} \end{indent}}
  
%\setlength{\topsep}{2mm}<
%\setlength{\parindent}{15pt}

%\newlength{\OBLS}\setlength{\OBLS}{\baselineskip}
%\newlength{\OPAR}\setlength{\OPAR}{\parindent}
%\newcommand{\pgskip}{\hspace{\mathindent}\=\+}
%\newcommand{\FOR}[1]{\FRM{\,#1\,}}
%\newcommand{\FRM}[1]{\mbox{$#1$}}
%\newcommand{\DERIVE}
%  {\setlength{\baselineskip}{1.3\baselineskip}
%  \begin{tabbing}
%  \hspace{\OPAR}\=\hspace{\OPAR}\=\hspace{\OPAR}\=\hspace{\OPAR}\=\hspace{\OPAR}\=\hspace{\OPAR}\=\kill
%  }
%\newcommand{\form}[1]
%  {\>\>\FOR{#1}}
%\newcommand{\hint}[2]
%  {\\*\>\FRM{#1}\>\>\{#2\}\\}
%\newcommand{\STARTPROOF}[1]
%  {\TOPROVE{#1}\\$\vartriangleright$ \>\>}
%\newcommand{\TOPROVE}[1]
%  {#1}
%\newcommand{\ENDDERIVE}
%  {\end{tabbing}\setlength{\baselineskip}{\OBLS}
%}
%\newcommand{\ENDPROOF}
%  {$\rbrack\arrowvert$\\
%  $\square$\\\\}
%\newcommand {\IND}
%  {\setlength{\parindent}{\OPAR}\\
%  \begin{indent}}
%\newcommand {\ENDIND}
%  {\end{indent}\\
%  \setlength{\parindent}{0 pt}\\}
%\newcommand {\IF}
%  {\mathbf{if} \:\:}
%\newcommand {\BAR}
%  {\ [\!] \:\:\:}
%\newcommand {\FI}
%  {\mathbf{fi} \:\:}
%\newcommand {\ARROW}
%  {\rightarrow}
%\newcommand{\clearemptydoublepage}{\newpage{\pagestyle{empty}\cleardoublepage}}
%\newcommand{\TT} {\textit{true}}
%\newcommand{\FF} {\textit{false}}


%%%%%%%%%%%%%%%%%%%% MACROS %%%%%%%%%%%%%%%%%%%%
\floatstyle{plain}
\newfloat{grammar}{thp}{lop}
\floatname{grammar}{Grammar}


%%%%%%%%%%%%%%%%%%%%%Stuff from SICL article%%%%%%
%\newcommand{\sos}[1]{\mathcal{#1}}
%\newcommand{\sset}[1]{\texttt{#1}}
%\newcommand{\restr}[2]{#1\restriction#2}

%\def\universe{A}
%\def\Empty{\mbox{\O}}





%%%%%%%%%%%%%%%%%%% DOCUMENT %%%%%%%%%%%%%%%%%%%
\begin{document}

\title{A case for delayed reductions in left-to-right parsing of context-free grammars}
\author{\textbf{Supervisors:} Derrick Kourie, Bruce Watson\\
\textbf{Author:} Rehno Lindeque (23123576)}

\maketitle

\begin{abstract}
In this paper we suggest the use of delayed actions for generalized parsing of unambiguous context-free grammars. 
Our contention is that delayed reductions in a left-to-right parser describes the nature of the parsing process in a more light-weight, intuitive manner than existing techniques.
It is also hoped that this might lead to simpler, more efficient parsers without sacrificing the generality of our grammars.
This work is therefore intended as preparatory material for further research in this area.
To this end we develop a set of tools for studying and developing the semantics of delayed parsing actions.
A small pseudo instruction set is shown which includes delayed actions used to resolve rules containing cycles in their lookahead sets.
Operational semantics for each of these instructions are provided and the implementation of an interpreter for the instruction set is demonstrated.
In addition a prototype construction algorithm is presented, capable of generating parsers for a limited subset of unambiguous grammars.
Finally we discuss the remaining problems with this technique and possible approaches to resolving them in future work.\\
\end{abstract}

\textbf{Keywords:} LD parsing, Recursive-ascent parsing, context-free grammar, generalized parsing, GLR parsing, LR parsing, LL parsing

\section{Introduction}

\subsection{Top-down vs Bottom-up parsing}

The subject of parsing is widely regarded to be a well-understood topic in computer science having been exposed to many years of study.
Two general strategies have been identified known as top-down and bottom-up parsing.\\

Top-down parsing techniques operate by deriving productions near the root of the grammar before attempting to derive productions further down the tree.
Such top-down parsers are often programmed by hand using a methodology called ``recursive descent'' parsing [citation needed].
A procedure is created for every production in the grammar and child productions are derived by calling their procedures in a recursive fashion.
This may be done in a speculative manner, backtracking where necessary, or by using some form of \emph{lookahead} mechanism where lexical tokens are investigated in advance before a node is created.\\

Bottom-up techniques on the other hand attempt to deduce the leaf nodes of the parse tree first before moving up to the root nodes. 
Theoretically this gives such a parser access to more contextual information [citation needed] as the derivation of productions are delayed until their contents has already been recognized.
However, a top-down parser may not always have enough information to derive a production even when the contents of such a production has been recognized. 
Certain context-free grammars require tokens following the the production to be recognized before it can decide whether the choice is really correct inside the context in which it is recognized
\footnote{This concept should not be confused with the class of grammars known as \emph{context-sensitive} grammars which may have more than one token on the left-hand side of production rules.}.
For this reason top-down parsers also typically employ lookahead in making such deductions.\\

In \cite{knuth65} Knuth developed a body of theoretical knowledge around the concept of ``left-to-right parsers with rightmost derivation'' known in short as \emph{LR parsers}. 
LR parsing captures the idea of directed, bottom-up parsing with a concrete algorithm.
Over time many variants of LR parsing, such as LALR [citation] and SLR [citation], have been developed as well as some optimizations [citation needed: Very Fast LR Parsing, Thomas J. Pennell].
When classifying a grammar for LR parsing we usually refer to an \emph{LR(k)} grammar where $k$ indicates some fixed \emph{lookahead} value, allowing the parser to look forward in the stream of input tokens when deciding on the next action to take.
However, the traditional approach to generating LR parsers with $k > 2$ has proven prohibitively difficult, leading most tools that employ LR or LALR techniques to restrict lookahead to very small values
\footnote{Typical lookahead used in LR(k) or LALR(k) parser generators has \emph{k = 1} or \emph{k = 2}.}.
Despite this limitation these tools produce some of the most efficient parsers and remain a popular choice amongst users.\\

LL parsers or ``left-to-right parsers with left-most derivation'' are the directed top-down analog to LR parsers.
Similar to \emph{LR(k)} grammars, a grammar may also be classified in the class \emph{LL(k)}. 
Once again \emph{k} indicates the minimum number of tokens that an LL parser would need to look ahead in order to recognize all the strings generated by this grammar.\\

Recursive ascent parsing, considered to be an analog to recursive descent parsing for bottom-up parsing %\cite{13326, 47909, 770849}.
In a similar manner procedures corresponding to productions are called recursively and every procedure returns the production it deduced to its caller, allowing the caller to decide on its next action.
Recursive ascent has been advocated as a learning exercise or teaching LR parsing. %\cite{13326, 47909, 770849}.
The parsing strategy presented in this paper, although somewhat different from a technical point of view, shows some commonality with recursive ascent parsing.

\emph{(TODO)\\ %BUSY HERE
Recursive ascent parsers \cite{13326, 47909, 770849} }

\subsection{Generalized parsing approaches}

Several techniques have shown promise in delivering more general parsing approaches.
Specifically context-free languages, which include grammars with unbounded lookahead and ambiguous grammars, has seen plenty of research.
In order to 

Generalized parsing techniques are most often used in the domain of natural language processing while programming language tools have traditionally stuck to LR, LALR and certain LL parsing tools.
It has been shown that the strings produced by any LR(k) grammar can be recognized using an LR(1) grammar [todo: citation].
However, as \cite{Parr95lland} suggests, a distinction could be made between \emph{language recognition} and \emph{language translation}. 
Indeed, many parser generators will simplify the output by removing nodes which do not. 
This it itself lends support for the idea powerful parsing tools can make use of simpler grammars which also result in simpler parse trees.

... the parser must \emph{look ahead} into the input stream in order to derive the correct rule to reduce.

It has been claimed in the past that computer languages should be designed for weaker parsers as this also makes it easier to read and understand for a human.
We will not challenge this idea in our paper although we do suggest that further research should be conducted in order to verify such a claim.

\emph{(TODO)\\ %BUSY HERE
Motivation: Translation versus recognition (why k $>$ 1 lookahead is needed) \cite{Parr95lland}}


In theory LR(k) parser has been shown to be more powerful than their equivalent LL(k) parsers because an LR parser uses more context information to derive reductions.
For this reason larger values of k is often desirable in practical LL parsers.
However, LL(k) parsing, long believed to be intractable for large values of k, has shown large improvements in performance through the use of deterministic finite automata. 
Additionally LL parsing has proven amenable to extension for larger classes of grammars than plain LR and LARL parsers can handle.
The popular ANTLR parser makes use of these techniques and has introduced two variants: LL(*) [citation needed]\cite{} and more recently Predicated-LL \cite{} [citation needed].
s
Despite significant improvements in the efficiency of GLR parsers, they are still regarded as slow in comparison to their traditional LALR counterparts despite equivalent algorithmic performance.
This is usually attributed to the cost of maintaining the stack-structured graph that gives such a parser the capability to merge different branches. [citation needed?]




\emph{(TODO)\\ %BUSY HERE
Generalized Parsing Techniques \cite{Thurston07}\\
Chart parsers: Earley $\rightarrow$ GLR \\
Right Nulled GLR Parsers \cite{1146810}\\
Performance of GLR Parser \cite{Mcpeak04elkhound:a}\\ %(Same asymptotic performance, but slower by about a factor of 10x according to \cite{Mcpeak04elkhound:a} due to complicated data structures (Stack Structured Graph)
%(todo: find a better reference?))\\
Previous work on delayed reductions (similar ideas, but rather different algorithms) \cite{1287949, Marc80, 991520, 146993}\\
%TODO:          Shift-Resolve Parsing: Simple,
%     Unbounded Lookahead, Linear Time∗
%Jos ́ Fortes G ́lvez
%   e            a           Sylvain Schmitz            Jacques Farr ́e
%         Universidad de Las Palmas de Gran Canaria, Spain
%                        jfortes@dis.ulpgc.es
%     Laboratoire I3S, Universit ́ de Nice - Sophia Antipolis, France
%                               e
%          schmitz@i3s.unice.fr, Jacques.Farre@unice.fr
Parsing with continuations
}




\subsection{Overview}
\emph{(TODO: Expand + look at phrasing)}
This remainder of this paper will be structured as follows. 
We begin by describing the basic ideology behind using delayed parsing actions along with some basic examples of their utility.
Next, the theoretical aspects of the parser is explored along with formal definitions and proofs.


\section{Delayed reductions}

\subsection{Basic strategy}

There are three aspects of the parsing process that we are interested in and we will refer to each aspect by the following names.
\begin{itemize}
\item Recognition---The ability of a parser to recognize various sequences of tokens according to some grammar without providing any output other than an assertion of the correctness of the syntax of its input and possibly an indication of where an error might be located in the source.
\item Translation---In the context of parsing translation refers to the process of converting the source program from its original representation to the final output of the parser. The output of the parser is usually in the form of a \emph{concrete syntax tree} also commonly referred to as a \emph{parse tree}.
\item Construction---The construction of a parser is handled by a separate algorithm responsible for automatically generating executable parsing code from the static grammar definition supplied by the user.
\end{itemize}

\emph{TODO: Grammar definitions. Production, rule etc)}\\
\emph{TODO: Perhaps redefine "production rule", or at least specify what it means to "recognize a production rule"}\\

Our basic strategy for parsing text proceeds in two passes.
The first is a \emph{recognition pass} with the additional constraint that production rules are recorded, corresponding to recognized sequences of lexical tokens. The rule tokens are placed into a temporary buffer---the \emph{rules buffer}---that grows in a stack-like manner, but also supports constant-time random-access operations.
In the second pass---the \emph{translation pass}---the sequence of recognized production rules are used to build the final parse tree in a similar a manner to an LR parser, but without any recognition steps or lookahead.\\

By separating these two aspects into distinct passes, the recognition pass is allowed the convenience of identifying the right hand sides of production rules in a more flexible order. 
However, the translation pass requires the rules buffer it is given to be correctly ordered in order to build the parse tree.
For this purpose, the mechanism used to reorder rules relies on placeholder tokens in the rules buffer representing unresolved production rules.
Whenever the parser must deduce a rule that cannot be fully determined, a placeholder token is appended to the rules buffer in its place. 
Once the rule can be resolved, the placeholder token is replaced with the correct rule number. 
In order to keep track of placeholders we also record their indexes in a separate stack, allowing the parser to return to these tokens later on and replace them using random-access writes.


Due to placeholder representing We will also refer to placeholder tokens as `delayed rules'...\\


In effect, placeholders are used in order to delay reductions until some unspecified lookahead has been recognized.
Clearly the order of rule recognition is relaxed using this mechanism. However, it is not completely unrestricted. 
The following conditions specify certain constraints by which the recognition pass must abide.

\begin{itemize}
\item As rules are recognized they are placed in the rules buffer in a left-to-right order. This corresponds with a bottom-up parsing strategy since production rules that are closer to the leaf nodes in the parse tree will normally be recognized before their parent rules are recognized.
\item Delayed rules may only be resolved in a right-to-left order, corresponding with the top-down strategy of parsing since delayed rules near the root of the tree must be resolved before 
rules near the leaf nodes can be resolved. 
It is worth noting however that, although the unresolved rules will only be resolved in a right-to-left order, rules may be added to the unresolved stack at any time which intuitively
allows the right-to-left rule resolution loop to be nested inside the main left-to-right recognition loop.
\end{itemize}

In order to explain this strategy in more formal terms we will define an instruction set and interpreter which in essence forms a tiny domain specific language for parsing with delayed actions.
For the sake of simplicity we begin by describing a simple modified LR(0) algorithm. This LR(0) algorithm has three basic operations: shift, reduce and switch. 
The shift and switch operations are responsible for recognizing terminal tokens on the input stream whereas the reduce action is responsible for translating strings of terminal tokens into nonterminal tokens.

An interpreter can be built to correctly execute these instructions. 
Using this approach we demonstrate the operation of a parser constructed using this set of actions.
The following free variables will be used to represent the state of the parser interpreter during execution:

\begin{itemize}
\item $input_{[0, input.length)}$ is an array of input tokens of length $input.length$ returned by a lexer.
\item $i$ is an index into the array of input tokens representing the current position of the parser in the stream during the recognition pass such that $input_i$ is the token currently being tested.
This index will never be decreased and need only increase in increments of 1.
\item $actions_{[0, actions.length)}$ is a sequence of instructions representing the encoded parser similar to the source code that is output by parser generators. 
This array is automatically generated by a separate parser construction algorithm.
\item $j$ is an index into the array of actions such that $actions_j$ refers to the current instruction being decoded.
\item $outputrules$ is a dynamically growing array storing the output of recognition pass.
\item $k$ is an index into the array of output rules such that $k$ always points to the first unfilled element of the array.
\item $callstack$ is a dynamically growing stack for storing the current index of the parser in the instruction table in order to return to it later.
\item $l$ is an index into the call stack such that $l$ always points to the first unfilled element of the stack.
%\item $continuations$ is a stack for storing the current index of the parser in the instruction table in order to return to it later.
\item $error$ is a special error state which (in the absence of error recovery) will halt the parser and report a syntax error.
\end{itemize}

Every action in the array will be represented using the format\\ \texttt{ action [( [parameters\textellipsis] )]}\\

Operational semantics for the following operations is discussed below, followed by the implementation of a basic LR(0) parser.
\begin{itemize}
\item \texttt{shift(terminaltoken)}
\item \texttt{switch(terminaltoken $\mapsto$ state, \textellipsis)}
\item \texttt{reduce(rule)}
\end{itemize}

\subsubsection{The `shift' action}
\texttt{shift(terminaltoken)}\\
The shift action is used to step over input tokens deterministicly. 
There are only two options: Either the token matches the parameter given to the shift action, or the parser is put into an error state.

\begin{equation}
\infer{i, j \mapsto i+1, j+1}{input_i = a & actions_j = shift(a)} \tag{Def 1.1}
\end{equation}

\begin{equation}
\infer{parserstate \mapsto error}{input_i = a & actions_j = shift(b)} \tag{Def 1.2}
\end{equation}\\

\subsubsection{The `switch' action}
\texttt{switch(terminaltoken $\mapsto$ state, \textellipsis)}\\
The switch action is used to recognize input tokens nondeterministicly.
The set of parameters given to the switch action can be viewed as an associative map of key-value pairs where a terminal token maps to a parse state or, in other words, an index into the generated parser code.
If a terminal token on the input stream is contained in the map then the switch action will cause the parser to jump to the new state similar to how switch statements behave in many programming languages.
If the input terminal is not recognized by the switch action, then the parser is placed in the $error$ state. 
In the definition below the function $getvalue$ returns the state index value corresponding to the terminal token given as the key.

\begin{equation}
\infer{i, j \mapsto i+1, getvalue(A, input_i)}{input_i \in A & actions_j = switch(A)} \tag{Def 1.3}
\end{equation}

\begin{equation}
\infer{parsestate \mapsto error}{input_i \notin A & actions_j = switch(A)} \tag{Def 1.4}
\end{equation}\\

There is an apparent redundancy between the switch and the shift actions. 
A shift action of the form $shift(a)$ will behave similar to a switch action in the form $switch([a \mapsto next])$ where $next$ indicates the index of the instruction that directly follows the switch.
However, a distinction is made between these two actions as a matter of convenience: 
We view switch actions as the primary recognition mechanism of the parser, because they involve a non-trivial choice.
By contrast, shift actions do not make any significant contribution to the internal state of the parser other than to catch syntax errors.
	
\subsubsection{The `reduce' action}
\texttt{reduce(rule)}\\
The parameter given to the reduce action is an index into a list of all rules in the given grammar where every rule is in the form \texttt{nonterminaltoken $\rightarrow$ terminaltoken \textellipsis}
A production in a grammar may be reduced from any number of different sequences of terminal tokens, however a rule recognizes only one possible sequence of terminal tokens.
In other words, given some rule both the left and right side of the reduction can be determined (although identifiers must still be stored).
For this reason we use rule numbers internally rather than using grammar productions directly.

\begin{equation}
\infer{j, outputrules_k, k \mapsto j+1, r, k+1}{actions_j = reduce(r)} \tag{Def 1.5}
\end{equation}\\

\subsubsection{Basic LR(0) parsing}

Since LR(0) grammars requires no lookahead they can be parsed directly using only recognition and reduction actions.
To see how these rules translate into a familiar language we construct an LR(0) parser for a simple grammar in the guarded command language (GCL) in a similar style to a recursive-ascent parser.

This simple grammar will be used for illustration, with $S$ representing the starting token.

\begin{align*}
G \equiv \quad & A_1 \rightarrow a\\
               & B_1 \rightarrow b\\
               & S_1 \rightarrow x A \$\\
               & S_2 \rightarrow x B \$\\
               & S_3 \rightarrow x y \$
\end{align*}

In order to recognize this grammar we will derive the following sequence of parsing steps:
\begin{enumerate}
\item The first token to be recognized must be $x$. Since the choice of $x$ is predetermined a shift action, $shift(x)$, may be used which also checks for an error in the input stream.
%\begin{figure}[htbp]
\begin{center}
\begin{gcl}
\IF input_i = x \rightarrow i \becomes i + 1;
\BAR input_i \neq x \rightarrow parserstate \becomes error;
\FI
\end{gcl}
\end{center}
%\end{figure}

\item Now the set of valid tokens to follow are $\{ a, b, y \}$. 
Since this recognition involves a choice of $a$, $b$ or $y$ a logical branch occurs in the parser, indicating that a $switch$ action must be employed.
The choice of branch will place the parser into one of three distinct states. Hence it makes sense to indicate this by constructing a separate procedure for each.
%\begin{figure}[htbp]
\begin{center}
\begin{gcl}
\IF input_i = a \rightarrow 
                \qquad i \becomes i+1; 
                \qquad ParseS1(input, i);
\BAR input_i = b \rightarrow 
                \qquad i \becomes i+1;
                \qquad ParseS2(input, i);
\BAR input_i = y \rightarrow 
                \qquad i \becomes i+1; 
                \qquad ParseS3(input, i);
\BAR input_i \notin \{ a, b \} \rightarrow 
                \qquad parserstate \becomes error;
\FI
\end{gcl}
\end{center}
%\end{figure}

\item For each branch of the switch the remaining parsing actions are fully deterministic as no more choices remain.
The first two branches must output their corresponding reduction rules $A_1$ or $B_1$. 
For example, the reduction $reduce(A_1)$ may be performed as follows:
%\begin{figure}[htbp]
\begin{center}
\begin{gcl}
outputrules_k \becomes A_1;
k \becomes k + 1;
\end{gcl}
\end{center}
%\end{figure}

\item Now all that remains in each branch is to recognize the remaining `end-of-stream' token $\$$ and then output $S_1$, $S_2$ or $S_3$. 
The complete pseudo code that describes this process is listed below.

\begin{center}
\begin{gcl}
\PROC ParseS1(input)
outputrules_k \becomes A_1;
k \becomes k + 1;
\IF input_i = \$ \rightarrow 
                 \qquad i \becomes i + 1;
\BAR input_i \neq \$ \rightarrow 
                 \qquad parserstate \becomes error;
                 \qquad \textbf{return};
\FI
outputrules_k \becomes S_1;
k \becomes k + 1;
\CORP
\end{gcl}
\end{center}

\begin{center}
\begin{gcl}
\PROC ParseS2(input)
outputrules_k \becomes B_1;
k \becomes k + 1;
\IF input_i = \$ \rightarrow i \becomes i + 1;
\BAR input_i \neq \$ \rightarrow
                 \qquad parserstate \becomes error;
                 \qquad \textbf{return};
\FI
outputrules_k \becomes S_2;
k \becomes k + 1;
\CORP
\end{gcl}
\end{center}

\begin{center}
\begin{gcl}
\PROC ParseS3(input)
\IF input_i = \$ \rightarrow i \becomes i + 1;
\BAR input_i \neq \$ \rightarrow 
                 \qquad parserstate \becomes error;
                 \qquad \textbf{return};
\FI
outputrules_k \becomes S_3;
k \becomes k + 1;
\CORP
\end{gcl}
\end{center}

\clearpage
\begin{figure}[!ht]
\begin{center}
\begin{gcl}
\PROC ParseG(input, output, parserstate)
i \becomes 0;
\IF input_i = x \rightarrow i \becomes i + 1;
\BAR input_i \neq x \rightarrow 
                \qquad parserstate \becomes error;
                \qquad \textbf{return};
\FI
\IF input_i = a \rightarrow
                \qquad i \becomes i+1;
                \qquad ParseS1(input, i);
\BAR input_i = b \rightarrow 
                \qquad i \becomes i+1;
                \qquad ParseS2(input, i);
\BAR input_i = y \rightarrow 
                \qquad i \becomes i+1;
                \qquad ParseS3(input, i);
\BAR input_i \notin \{ a, b \} \rightarrow 
                \qquad parserstate \becomes error; 
                \qquad \textbf{return};
\FI
\CORP
\end{gcl}
\caption{An LR(0) parser example explained in pseudo code.}
\end{center}
\end{figure}

\end{enumerate}

\subsection{Delayed actions}
While the three operations discussed provide the necessary tools to recognize any non-recursive LR(0) grammar, they do not provide any manner of looking ahead into the input stream. Furthermore parsers using these instruction will exhibit exponential growth if no explicit `return' action is available.
Some additional actions are provided to resolve these issues:
\begin{itemize}
\item \texttt{delay}
\item \texttt{resolve(rule)}
\item \texttt{return}
\item In addition the \texttt{switch} action is slightly modified to use the callstack.
\end{itemize}

\subsubsection{The `delay' action}
\texttt{delay}\\
The delay action adds a placeholder token called $ignore$ to the output rules which may be replaced later.
In addition the index of the placeholder is pushed onto the stack of delays.
Note that the value of $ignore$ should be reserved so that it will not conflict with any rule number.
\begin{equation}
\infer{delays_l, l, j, outputrules_k, k \mapsto j, l+1, j+1, ignore, k+1}{actions_j = delay} \tag{Def 1.6}
\end{equation}\\

\subsubsection{The `resolve' action}
\texttt{resolve(rule)}\\
The resolve action is similar to the reduce action, but instead of immediately pushing a reduction rule onto the output stack it instead replaces the placeholder token previously pushed by a delay action.
It may also push the special token $ignore$ meaning that the output rules should remain unchanged and no reduction is necessary. 
Since the resolve action will pop the index of this placeholder token from the stack, this delay would not be re-evaluated again.
\begin{equation}
\infer{l, j, outputrules_{delays_l} \mapsto l-1, j+1, ignore}{actions_j = resolve(r)} \tag{Def 1.7}
\end{equation}\\

The pair of actions, `resolve' and `delay', works together in order to implement \emph{delayed reductions}. 
It is easy to see that the action $delay$ immediately followed by the action $resolve(r)$ is identical to our original action $reduce(r)$ and could be used as a replacement\footnote{We've chosen to keep the original reduce action because, in practice, programming language grammars often require very few lookaheads.}.

\subsubsection{The `return' action}
A $return$ action is needed in order to return to the state from which the previous $switch(...)$ action jumped. 
From a technical point of view, this allows a parser constructor to merge branches together, thus avoiding an exponential growth in the number of possible states. 
Indeed this is a necessary condition for parsers that must handle recursive grammars.

\begin{equation}
\infer{j, l \mapsto callstack_l, l-1}{actions_j = return} \tag{Def 1.9}
\end{equation}

\subsubsection{A modification of the `switch' action}
In order for the before mentioned action to work correctly we must also modify our original $switch$ action to record the state from which we've come.
The current parser state index will be pushed onto a call stack to be returned to later.

\begin{equation}
\infer{i, j, callstack_l, l \mapsto i+1, getvalue(A, input_i), j+1, l+1}{input_i \in A & actions_j = switch(A)} \tag{Def 1.8}
\end{equation}

However error checking remains the same as before.
\begin{equation}
\infer{parsestate \mapsto error}{input_i \notin A & actions_j = switch(A)} \tag{Def 1.4}
\end{equation}

\subsection{Traces}
To see how the delay/resolve pair can be used to look ahead in the input stream we must investigate several different forms of grammar rules.
In order to do this we must generate all possible input strings for each form. 
The concept of traces is used to show the actions that the parser must take for every possibility. 
A trace is simply a sequence of actions that the parser must take to recognize a particular instance of a valid input stream for the given grammar.
The advantage of this approach is that control flow can be ignored in order to focus exclusively on analysing which permutations of parsing actions produce equivalent output.
A mechanical method of generating traces is used such that every possibility could be enumerated, although one should be aware that the trace itself may not be finite.\\

In the following examples capitalized tokens represent nonterminals with subscripts differentiating between nonterminals produced by different rules. 
Lowercase tokens represent terminals and $S$ is always the starting nonterminal. 
Finally, $\$$ represents the special `end-of-stream' terminal indicating that no more input tokens remain.\\

%%%%%%%%%%%%% Example 1: Reductions required a fixed size lookahead
\subsubsection{Reductions requiring a fixed size lookahead}
\begin{tabular}[t]{cl}
\parbox{.3\textwidth}{
\begin{align*}
G_1 \equiv \quad & A_1 \rightarrow x\\
                 & B_1 \rightarrow x\\
                 & S_1 \rightarrow A a \$\\
                 & S_2 \rightarrow B b \$
\end{align*}}
\parbox{.8\textwidth}{This is a simple grammar requiring a fixed size lookahead in order to output one of the two production rules, $A_1$ or $B_1$.}
\end{tabular}

Suppose that this grammar was to be parsed by an LR(k) parser with a sufficiently large lookahead k. 
We could write down all the possible traces for this parser using our `shift', `reduce' and `switch' actions while ignoring any possible lookahead actions that must take place as follows:

\parbox{.25\textwidth}{\begin{align*}
&traces(G_1) \equiv \\
\{ & \langle shift(x), reduce(A_1), switch(a), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), reduce(B_1), switch(b), shift(\$), reduce(S_2) \rangle \}
\end{align*}}

Next we will exploit the fact that ordering of shift and switch actions relative to reduce actions is not enforced in our parsing methodology. This because a parse tree is only constructed during the second pass over the output reduction rules.
This really implies that any recognition action directly followed by a reduce action may swap places. 
Hence a trace $\langle shift(...), reduce(...) \rangle$ may be rewritten as $\langle reduce(...), shift(...) \rangle$ and similarly for switch statements $\langle switch(...), reduce(...) \rangle$ may be rewritten as $\langle reduce(...), switch(...) \rangle$.
We apply this rule selectively to the above trace in ordser to produce an equivalent trace:

\parbox{.3\textwidth}{\begin{align*}
&traces(G_1) \equiv \\
\{ & \langle shift(x), switch(a), reduce(A_1), shift(\$), reduce(S_1) \rangle, \\
   & \langle shift(x), switch(b), reduce(B_1), shift(\$), reduce(S_2) \rangle \}
\end{align*}}

Take note that this equivalent trace requires no lookahead whatsoever! 
Because the ordering of recognition and reduction operations have been relaxed many instances where a fixed lookahead would have been required by classical algorithms no longer apply.

%%%%%%%%%%%%% Example 2: Optional reduction based on a fixed lookahead
\subsubsection{Optional reduction requiring a fixed size lookahead}
\begin{tabular}[t]{cl}
\parbox{.3\textwidth}{
\begin{align*}
G_2 \equiv \quad & A_1 \rightarrow x\\
                 & S_1 \rightarrow A a \$\\
                 & S_2 \rightarrow x b \$
\end{align*}}
\parbox{.8\textwidth}{In this grammar the rule $A_1$ may be output, or the possible reduction could be ignored if $b$ is in the lookahead.}
\end{tabular}

This grammar produces a similar trace as before, but with the first reduction omitted in the rule $S_2$.

\parbox{.3\textwidth}{\begin{align*}
&traces(G_2) \equiv \\
\{ & \langle shift(x), reduce(A_1), switch(a), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), switch(b), shift(\$), reduce(S_2) \rangle \}
\end{align*}}

Once again the need for lookahead can be circumvented by simply shuffling the order of operations.

\parbox{.3\textwidth}{\begin{align*}
&traces(G_2) \equiv \\
\{ & \langle shift(x), switch(a), reduce(A_1), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), switch(b), shift(\$), reduce(S_2) \rangle \}
\end{align*}}

%%%%%%%%%%%%% Example 3: Disjoint reductions that require lookahead
\subsubsection{Disjoint reductions that require lookahead}
Reductions in different branches of a grammar's rules do not necessarily consume the same number of tokens. 
In addition the prefix of their recognized token strings may be the same, which requires a lookahead to be performed.\\
\begin{tabular}[t]{cl}
\parbox{.3\textwidth}{
\begin{align*}
G_3 \equiv \quad & A_1 \rightarrow x y\\
                 & B_1 \rightarrow y b\\
                 & C_1 \rightarrow x\\
                 & S_1 \rightarrow A a \$\\
                 & S_2 \rightarrow C B \$
\end{align*}}
\parbox{.8\textwidth}{The rules $A_1$ and $C_1$, $B_1$ in this grammar are partially disjoint.}
\end{tabular}

Once the lexical token $x$ has been seen, the parser must first look ahead 2 tokens before 
deciding whether the string $x y$ belongs to $A_1$ or to $C_1$ followed by $B_1$.

\parbox{.3\textwidth}{\begin{align*}
&traces(G_3) \equiv \\
\{ & \langle shift(x), shift(y), reduce(A_1), switch(a), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), reduce(C_1), shift(y), switch(b), reduce(B_1), shift(\$), reduce(S_2) \rangle \}
\end{align*}}

This trace may also be reordered without any need for lookahead.

\parbox{.3\textwidth}{\begin{align*}
&traces(G_3) \equiv \\
\{ & \langle shift(x), shift(y), switch(a), reduce(A_1), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), shift(y), switch(b), reduce(C_1), reduce(B_1), shift(\$), reduce(S_2) \rangle \}
\end{align*}}

%%%%%%%%%%%%% Example 4: Right recursive reductions with outer lookahead
\subsubsection{Right recursive reductions with outer lookahead}
In the previous examples, the traces could simply be reordered in order to obviate the need for lookahead, however this is no longer possible in the presence of recursive grammar rules.
Such a recursive grammar with a lookahead outside of its cycle no longer falls into the class of LR(k) grammars since k is unbounded in these instances.

\begin{tabular}[t]{cl}
\parbox{.3\textwidth}{
\begin{align*}
G_4 \equiv \quad & A_1 \rightarrow x\\
                 & A_2 \rightarrow x A\\
                 & B_1 \rightarrow x\\
                 & B_2 \rightarrow x B\\
                 & S_1 \rightarrow A a \$\\
                 & S_2 \rightarrow B b \$
\end{align*}}
\parbox{.8\textwidth}{The rules $A_2$ and $B_2$ in this grammar are both in a form of \emph{tail recursion}. 
We will also call this \emph{right recursion} in this paper in order to emphasize the contrast with \emph{left recursion}.}
\end{tabular}

\parbox{.3\textwidth}{\begin{align*}
&traces(G_4) \equiv \\
\{ & \langle shift(x), reduce(A_1), switch(a), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), shift(x), reduce(A_1), reduce(A_2), switch(a), shift(\$), reduce(S_1) \rangle, \textellipsis \\
   & \langle shift(x), reduce(B_1), switch(b), reduce(S_2) \rangle,\\
   & \langle shift(x), shift(x), reduce(B_1), reduce(B_2), switch(b), shift(\$), reduce(S_2) \rangle, \textellipsis \}
\end{align*}}

While we could attempt to shuffle the operations in these traces so that reductions are only made after the tokens $a$ and $b$ have been recognized, 
additional effort would be required on the part of the parser to remember the number and nature of reductions to apply. 
By using a $return$ action we can determine which rules should be output, however the rules should also be applied in the correct order to produce the correct trace. 
We will discuss this more fully in the following section.
Suffice to say, reductions could be recognized in a reversed (top-down) order but needs to be output in the original (bottom-up) ordering.
However our $delay/resolve$ mechanism will allow us to do exactly this by placing placeholders in the output stream and later replacing them in right-to-left order using $resolve$ actions.
Hence, the equivalent trace can be written as follows:

\parbox{.3\textwidth}{\begin{align*}
&traces(G_4) \equiv \\
\{ & \langle shift(x), delay, switch(a), resolve(A_1), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), shift(x), delay, delay, switch(a), resolve(A_2), resolve(A_1), shift(\$), reduce(S_1) \rangle, \textellipsis \\
   & \langle shift(x), delay, switch(b), resolve(B_1), shift(\$), reduce(S_2) \rangle,\\
   & \langle shift(x), shift(x), delay, delay, switch(b), resolve(B_2), resolve(B_1), shift(\$), reduce(S_2) \rangle, \textellipsis \}
\end{align*}}

Notice the reverse ordering of the actions $\langle resolve(A_2), resolve(A_1) \rangle$ in contrast to the original $\langle reduce(A_1), reduce(A_2) \rangle$.
A left recursive rule with outer lookahead may be resolved in much the same manner as a right recursive rule.

%%%%%%%%%%%%% Example 5: Combined left and right recursive grammar
\subsubsection{Combined left and right recursive grammar with outer lookahead}
\begin{tabular}[t]{cl}
\parbox{.3\textwidth}{
\begin{align*}
G_5 \equiv \quad & A_1 \rightarrow x\\
                 & A_2 \rightarrow x A\\
                 & B_1 \rightarrow x\\
                 & B_2 \rightarrow B x\\
                 & S_1 \rightarrow A a \$\\
                 & S_2 \rightarrow B b \$
\end{align*}}
\parbox{.8\textwidth}{The rules $A_2$ and $B_2$ are both partially disjoint and recursive. One is left recursive in this case while the other is right recursive and the combination
requires a look ahead outside of the cycle.}
\end{tabular}

\parbox{.3\textwidth}{\begin{align*}
&traces(G_5) \equiv \\
\{ & \langle shift(x), delay, switch(a), resolve(A_1), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), shift(x), delay, delay, switch(a), resolve(A_2), resolve(A_1), shift(\$), reduce(S_1) \rangle, \textellipsis \\
   & \langle shift(x), delay, switch(b), resolve(B_1), shift(\$), reduce(S_2) \rangle,\\
   & \langle shift(x), shift(x), delay, delay, switch(b), resolve(B_2), resolve(B_1), shift(\$), reduce(S_2) \rangle, \textellipsis \}
\end{align*}}

An equivalent trace with delays can be written using delay/resolve actions as follows:

\parbox{.3\textwidth}{\begin{align*}
&traces(G_5) \equiv \\
\{ & \langle shift(x), delay, switch(a), resolve(A_1), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), shift(x), delay, delay, switch(a), resolve(A_2), resolve(A_1), shift(\$), reduce(S_1) \rangle, \textellipsis \\
   & \langle shift(x), delay, switch(b), resolve(B_1), shift(\$), reduce(S_2) \rangle,\\
   & \langle shift(x), shift(x), delay, delay, switch(b), resolve(B_2), resolve(B_1), shift(\$), reduce(S_2) \rangle, \textellipsis \}
\end{align*}}

%%%%%%%%%%%%% Example: 
%\subsubsection{}
%\begin{tabular}[t]{cl}
%\parbox{.3\textwidth}{
%\begin{align*}
%G_6 \equiv \quad & A_1 \rightarrow x\\
%                 & B_1 \rightarrow x\\
%                 & C_1 \rightarrow y\\
%                 & C_2 \rightarrow A C a\\
%                 & C_3 \rightarrow B C b\\
%                 & S_1 \rightarrow C \$
%\end{align*}}
%\parbox{.8\textwidth}{sfd}
%\end{tabular}
%
%\parbox{.3\textwidth}{\begin{align*}
%&traces(G_6) \equiv \\
%\{ & \langle switch(y), reduce(C_1), shift(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(A_1), switch(y), reduce(C_1), switch(a), reduce(C_2), shift(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(B_1), switch(y), reduce(C_1), switch(b), reduce(C_3), shift(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(A_1), switch(x), reduce(A_1), switch(y), reduce(C_1), switch(a), reduce(C_2), \\&\qquad switch(a), reduce(C_2), shift(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(A_1), switch(x), reduce(B_1), switch(y), reduce(C_1), switch(b), reduce(C_3), \\&\qquad switch(a), reduce(C_2), shift(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(B_1), switch(x), reduce(A_1), switch(y), reduce(C_1), switch(a), reduce(C_2), \\&\qquad switch(b), reduce(C_3), shift(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(B_1), switch(x), reduce(B_1), switch(y), reduce(C_1), switch(b), reduce(C_3), \\&\qquad switch(b), reduce(C_3), shift(\$), reduce(S_1) \rangle,\\
%   & \textellipsis \}
%\end{align*}}
%
%An equivalent trace with delays can be written using delay/resolve actions as follows:
%
%\parbox{.3\textwidth}{\begin{align*}
%%\begin{equation*}
%&traces(G_6) \equiv \\
%\{ & \langle switch(y), reduce(C_1), shift(\$), reduce(S_1) \rangle,\\ 
%%\begin{array}{l}
%   &  \langle switch(x), delay(A,B), switch(y), reduce(C_1), switch(a), resolve(A), reduce(C_2), shift(\$), \\&\qquad reduce(S_1) \rangle,\\
%   &  \langle switch(x), delay(A,B), switch(y), reduce(C_1), switch(b), resolve(B), reduce(C_3), shift(\$), \\&\qquad reduce(S_1) \rangle,\\
%   &  \langle switch(x), delay(A,B), switch(x), delay(A,B), switch(y), reduce(C_1), switch(a), resolve(A), \\&\qquad reduce(C_2), switch(a), resolve(A), reduce(C_2), shift(\$), reduce(S_1) \rangle,\\
%   &  \langle switch(x), delay(A,B), switch(x), delay(A,B), switch(y), reduce(C_1), switch(b), resolve(B), \\&\qquad reduce(C_3), switch(a), resolve(A), reduce(C_2), shift(\$), reduce(S_1) \rangle,\\
%   &  \langle switch(x), delay(A,B), switch(x), delay(A,B), switch(y), reduce(C_1), switch(a), resolve(A), \\&\qquad reduce(C_2), switch(b), resolve(B), reduce(C_3), shift(\$), reduce(S_1) \rangle,\\
%   &  \langle switch(x), delay(A,B), switch(x), delay(A,B), switch(y), reduce(C_1), switch(b), resolve(B), \\&\qquad reduce(C_3), switch(b), resolve(B), reduce(C_3), shift(\$), reduce(S_1) \rangle,\\
%   &  \textellipsis
%%\end{array}
%\}
%%\end{equation*}
%\end{align*}}

%%%%%%%%%%%%%% Example 7: 
%\subsubsection{}
%\begin{tabular}[t]{cl}
%\parbox{.3\textwidth}{
%\begin{align*}
%G_7 \equiv \quad & A_1 \rightarrow x\\
%                 & B_1 \rightarrow x\\
%                 & C_1 \rightarrow y\\
%                 & C_2 \rightarrow C A a\\
%                 & C_3 \rightarrow C B b\\
%                 & S_1 \rightarrow C \$
%\end{align*}}
%\parbox{.8\textwidth}{sfd}
%\end{tabular}
%
%\parbox{.3\textwidth}{\begin{align*}
%&traces(G_7) \equiv \\
%\{ & \langle shift(y), reduce(C_1), switch(\$), reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), reduce(A_1), switch(a), reduce(C_2), switch(\$), reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), reduce(B_1), switch(b), reduce(C_3), switch(\$), reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), reduce(A_1), switch(a), reduce(C_2), switch(x), reduce(A_1), \\&\qquad switch(a), reduce(C_2), switch(\$), reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), reduce(B_1), switch(b), reduce(C_3), switch(x), reduce(A_1), \\&\qquad switch(a), reduce(C_2), switch(\$), reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), reduce(A_1), switch(a), reduce(C_2), switch(x), reduce(B_1), \\&\qquad switch(b), reduce(C_3), switch(\$), reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), reduce(B_1), switch(b), reduce(C_3), switch(x), reduce(B_1), \\&\qquad switch(b), reduce(C_3), switch(\$), reduce(S_1) \rangle,\\
%   & \textellipsis \}
%\end{align*}}
%
%The equivalent trace with delays can be written as follows:
%
%\parbox{.3\textwidth}{\begin{align*}
%&traces(G_7) \equiv \\
%\{ & \langle shift(y), reduce(C_1), switch(\$), reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), delay(A,B), switch(a), resolve(A_1), reduce(C_2), switch(\$), \\&\qquad reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), delay(A,B), switch(b), resolve(B_1), reduce(C_3), switch(\$), \\&\qquad reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), delay(A,B), switch(a), resolve(A_1), reduce(C_2), switch(x), \\&\qquad delay(A,B), switch(a), resolve(A_1), reduce(C_2), switch(\$), reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), delay(A,B), switch(a), resolve(A_1), reduce(C_2), switch(x), \\&\qquad delay(A,B), switch(b), resolve(B_1), reduce(C_3), switch(\$), reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), delay(A,B), switch(b), resolve(B_1), reduce(C_3), switch(x), \\&\qquad delay(A,B), switch(b), resolve(B_1), reduce(C_3), switch(\$), reduce(S_1) \rangle,\\
%   & \langle shift(y), reduce(C_1), switch(x), delay(A,B), switch(b), resolve(B_1), reduce(C_3), switch(x), \\&\qquad delay(A,B), switch(a), resolve(A_1), reduce(C_2), switch(\$), reduce(S_1) \rangle,\\
%   & \textellipsis \}
%\end{align*}}

%%%%%%%%%%%%% Example 8: 
%\subsubsection{}
%\begin{tabular}[t]{cl}
%\parbox{.3\textwidth}{
%\begin{align*}
%G_8 \equiv \quad & A_1 \rightarrow x\\
%                 & B_1 \rightarrow x\\
%                 & C_1 \rightarrow y\\
%                 & C_2 \rightarrow A C a\\
%                 & C_3 \rightarrow C B b\\
%                 & S_1 \rightarrow C \$
%\end{align*}}
%\parbox{.8\textwidth}{sfd}
%\end{tabular}
%
%\parbox{.3\textwidth}{\begin{align*}
%&traces(G_8) \equiv \\
%\{ & \langle switch(y), reduce(C_1), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(A_1), switch(y), reduce(C_1), switch(a), reduce(C_2), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(y), reduce(C_1), switch(x), reduce(B_1), shift(b), reduce(C_3), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(A_1), switch(x), reduce(A_1), switch(y), reduce(C_1), shift(a), reduce(C_2), shift(a), \\&\qquad reduce(C_2), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(A_1), switch(y), reduce(C_1), switch(x), reduce(B_1), switch(b), reduce(C_3), shift(a), \\&\qquad reduce(C_2), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(A_1), switch(y), reduce(C_1), switch(a), reduce(C_2), switch(x), reduce(B_1), shift(b), \\&\qquad reduce(C_3), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(y), reduce(C_1), switch(x), reduce(B_1), shift(b), reduce(C_3), switch(x), reduce(B_1), shift(b), \\&\qquad reduce(C_3), switch(\$), reduce(S_1) \rangle,\\
%   & \textellipsis \}
%\end{align*}}
%
%The equivalent trace with delays can be written as follows:
%
%\parbox{.3\textwidth}{\begin{align*}
%&traces(G_8) \equiv \\
%\{ & \langle switch(y), reduce(C_1), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(A_1), switch(y), reduce(C_1), switch(a), reduce(C_2), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(y), reduce(C_1), switch(x), reduce(B_1), shift(b), reduce(C_3), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(A_1), switch(x), reduce(A_1), switch(y), reduce(C_1), shift(a), reduce(C_2), shift(a), \\&\qquad reduce(C_2), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(A_1), switch(y), reduce(C_1), switch(x), reduce(B_1), switch(b), reduce(C_3), shift(a), \\&\qquad reduce(C_2), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(x), reduce(A_1), switch(y), reduce(C_1), switch(a), reduce(C_2), switch(x), reduce(B_1), shift(b), \\&\qquad reduce(C_3), switch(\$), reduce(S_1) \rangle,\\
%   & \langle switch(y), reduce(C_1), switch(x), reduce(B_1), shift(b), reduce(C_3), switch(x), reduce(B_1), shift(b), \\&\qquad reduce(C_3), switch(\$), reduce(S_1) \rangle,\\
%   & \textellipsis \}
%\end{align*}}

%S1{ C1{y} $ }
%S1{ C2{ A{x} C1{y} a} $ }
%S1{ C3{ C1{y} B{x} b } $ }
%S1{ C2{ A{x} C2{ A{x} C1{y} a } a } $ }
%S1{ C2{ A{x} C3{ C1{y} B{x} b } a } $ }
%S1{ C3{ C2{ A{x} C1{y} a } B{x} b } $ }
%S1{ C3{ C3{ C1{y} B{x} b } B{x} b } $ }

%(Surprisingly enough, this one needs no look-aheads)


%%%%%%%%%%%%% Example 9: Look ahead over a cycle
\subsubsection{Look ahead over a cycle}
\begin{tabular}[t]{cl}
\parbox{.3\textwidth}{
\begin{align*}
G_9 \equiv \quad & A_1 \rightarrow x\\
                 & B_1 \rightarrow x\\
                 & C_1 \rightarrow y\\
                 & C_2 \rightarrow C y\\
                 & S_1 \rightarrow A C a \$\\
                 & S_2 \rightarrow B C b \$
\end{align*}}
\parbox{.8\textwidth}{In this grammar the rules $A_1$ and $B_1$ are not recursive, however their lookahead includes a cycle in $C$. 
This type of grammar also requires a delayed reductions to resolve the choice of $A_1$  $B_1$.}
\end{tabular}

\parbox{.3\textwidth}{\begin{align*}
&traces(G_9) \equiv \\
\{ & \langle shift(x), reduce(A_1), shift(y), reduce(C_1), switch(a), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), reduce(B_1), shift(y), reduce(C_1), switch(b), shift(\$), reduce(S_2) \rangle,\\
   & \langle shift(x), reduce(A_1), shift(y), reduce(C_1), switch(y), reduce(C_2), switch(a), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), reduce(B_1), shift(y), reduce(C_1), switch(y), reduce(C_2), switch(b), shift(\$), reduce(S_2) \rangle,\\
   & \textellipsis \}
\end{align*}}

The equivalent trace with delays can be written as follows:

\parbox{.3\textwidth}{\begin{align*}
&traces(G_9) \equiv \\
%\{ & \langle shift(x), delay(A,B), shift(y), reduce(C_1), switch(a), resolve(A_1), shift(\$), reduce(S_1) \rangle,\\
%   & \langle shift(x), delay(A,B), shift(y), reduce(C_1), switch(b), resolve(B_1), shift(\$), reduce(S_2) \rangle,\\
%   & \langle shift(x), delay(A,B), shift(y), reduce(C_1), switch(y), reduce(C_2), switch(a), resolve(A), shift(\$), \\&\qquad reduce(S_1) \rangle,\\
%   & \langle shift(x), delay(A,B), shift(y), reduce(C_1), switch(y), reduce(C_2), switch(b), resolve(B), shift(\$), \\&\qquad reduce(S_2) \rangle,\\
\{ & \langle shift(x), delay, shift(y), reduce(C_1), switch(a), resolve(A_1), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), delay, shift(y), reduce(C_1), switch(b), resolve(B_1), shift(\$), reduce(S_2) \rangle,\\
   & \langle shift(x), delay, shift(y), reduce(C_1), switch(y), reduce(C_2), switch(a), resolve(A), shift(\$), \\&\qquad reduce(S_1) \rangle,\\
   & \langle shift(x), delay, shift(y), reduce(C_1), switch(y), reduce(C_2), switch(b), resolve(B), shift(\$), \\&\qquad reduce(S_2) \rangle,\\
   & \textellipsis \}
\end{align*}}

%S1 { A{x} C1{y} a }
%S2 { B{x} C1{y} b }
%S1 { A{x} C2{ C1{y} y } a }
%S2 { B{x} C2{ C1{y} y } b }


%%%%%%%%%%%%% Example 10: Looking ahead into a cycle
\subsubsection{Look ahead into a cycle}
\begin{tabular}[t]{cl}
\parbox{.3\textwidth}{
\begin{align*}
G_{10} \equiv \quad & A_1 \rightarrow x\\
                  & B_1 \rightarrow x\\
                  & C_1 \rightarrow a\\
                  & C_2 \rightarrow y C\\
                  & D_1 \rightarrow b\\
                  & D_2 \rightarrow y C\\
                  & S_1 \rightarrow A C \$\\
                  & S_2 \rightarrow B D \$
\end{align*}}
\parbox{.8\textwidth}{Some times the lookahead itself will be contained inside a cycle.
In this example the preceeding rules $A_1$ and $B_1$ depend on a lookahead within the recursive rules $C_2$ and $D_2$.}
\end{tabular}

\parbox{.3\textwidth}{\begin{align*}
&traces(G_{10}) \equiv \\
\{ & \langle shift(x), reduce(A_1), switch(a), reduce(C_1), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), reduce(B_1), switch(b), reduce(D_1), shift(\$), reduce(S_2) \rangle,\\
   & \langle shift(x), reduce(A_1), switch(y), switch(a), reduce(C_1), reduce(C_2), shift(\$), reduce(S_1) \rangle,\\
   & \langle shift(x), reduce(B_1), switch(y), switch(b), reduce(D_1), reduce(D_2), shift(\$), reduce(S_2) \rangle,\\
   & \textellipsis \}
\end{align*}}

The equivalent trace with delays can be written as follows:

\parbox{.3\textwidth}{\begin{align*}
&traces(G_{10}) \equiv \\
% \{ & \langle shift(x), delay(A, B), switch(a), reduce(C_1), resolve(A), shift(\$), reduce(S_1) \rangle,\\
%    & \langle shift(x), delay(A, B), switch(b), reduce(D_1), resolve(B), shift(\$), reduce(S_2) \rangle,\\
%    & \langle shift(x), delay(A, B), switch(y), switch(a), reduce(C_1), reduce(C_2), resolve(A), shift(\$), \\&\qquad reduce(S_1) \rangle,\\
%    & \langle shift(x), delay(A, B), switch(y), switch(b), reduce(D_1), reduce(D_2), resolve(B), shift(\$), \\&\qquad reduce(S_2) \rangle,\\
 \{ & \langle shift(x), delay, switch(a), reduce(C_1), resolve(A), shift(\$), reduce(S_1) \rangle,\\
    & \langle shift(x), delay, switch(b), reduce(D_1), resolve(B), shift(\$), reduce(S_2) \rangle,\\
    & \langle shift(x), delay, switch(y), switch(a), reduce(C_1), reduce(C_2), resolve(A), shift(\$), \\&\qquad reduce(S_1) \rangle,\\
    & \langle shift(x), delay, switch(y), switch(b), reduce(D_1), reduce(D_2), resolve(B), shift(\$), \\&\qquad reduce(S_2) \rangle,\\
    & \textellipsis \}
\end{align*}}

%S1 { A{x} C1{a} $ }
%S2 { B{x} D1{b} $ }
%S1 { A{x} C2{ y C1{a} } $ }
%S2 { B{x} D2{ y D1{b} } $ }

\subsection{A left-to-right parser with delayed reductions}
We've seen that the instruction set given could in theory be used to construct parsers for a large range of context-free grammars if we can show that such a parser will produce the traces given above.
In \cite{knuth65} Knuth showed that determining whether a grammar is in the class LR(k) for some unspecified k is undecidable in general. \emph{ (TODO: Discuss how this affects our work) }\\

\emph{TODO:
When calling procedures in a recursive-ascent parser we are intuitively traversing the grammar's state diagram in a bottom-up manner.
However, when returning from a function to a previous state we revert to a top-down traversal.
Hence traversing the call stack in an upward or a downward direction can be viewed as switching between top-down versus bottom-up recognition of the grammar. 
In more general terms, when the bottom-up technique cannot reduce a rule it become resolved in a top-down fashion later on by replacing the delayed placeholder token.}

%%%%%%%%%%%%%%%%%%% IMPLEMENTATION
\section{Implementation}
\subsection{Parser interpreter}

In our implementation a generated parser is executed by an interpreter for the instruction set that we developed in the previous section of the paper.
The basic interpreter is described here with most of the optimizations and certain technical details removed for the sake of clarity. 
Additional implementation details will be briefly described in the last part of this section along with the internal representation of the generated parsers.\\

Three procedures are responsible for the execution of the parser. 
The first is a call to the lexer which is responsible for tokenizing the text file into the correct format for our main parsing functions.
We will gloss over this step as it is outside the scope of this work, but any suitable lexer can be dropped in and used at this point.
Next the recognition pass is executed, which takes the stream of lexical tokens and outputs a stream of rules. 
The translation pass is then responsible for iterating through these rules in order to assemble a concrete syntax tree, the resulting output of the parsing operation.
\emph{TODO: what about syntax errors?}

\begin{figure}[!ht]
\begin{center}
\begin{gcl}
\PROC ParseText(parseTree, textStream)
LexicalAnalysis(lexStream, textStream);
RecognitionPass(rulesStream, lexStream);
TranslationPass(parseTree, rulesStream, lexStream, textStream);
\CORP
\end{gcl}
\caption{Three main stages of the interpreter.}
\end{center}
\end{figure}

The primary operations performed by the recognition pass involve decoding instructions from the generated parser and their application on the lex stream and the rules stream. 
Note that the generated parser instructions is encoded in a bitcode array whose internal format is described more fully later on.
Sizes of records in the bitcode stream may vary, but for presentation purposes we will assume that every record in the stream can be accessed using a simple subscript.
In reality iteration over the encoded instructions occurs in a more ad hoc manner.

\clearpage
\begin{figure}[!ht]
\begin{center}
\begin{gcl}
\PROC RecognitionPass(rulesStream, lexStream, bitcodeStream)
    i \becomes 0;
    \DO lexStream_i \neq TOKEN\_\$ \rightarrow
      \DO DecodeShiftOp(bitcodeStream_j, lexStream_i) \rightarrow
      \quad Append(rulesStream, token);
      \BAR DecodeReduceOp(bitcodeStream_j) \rightarrow
      j \becomes j + 1;
      \BAR DecodeResolveOp(bitcodeStream_j) \rightarrow
      j \becomes j + 1;
      \BAR DecodeDealayOp(bitcodeStream_j) \rightarrow
      j \becomes j + 1;
      \BAR DecodeReturnOp(bitcodeStream_j) \rightarrow
      j \becomes j + 1;
      \BAR DecodeAcceptOp(bitcodeStream_j) \rightarrow
      j \becomes j + 1;
    \OD
    i \becomes i + 1;
  \OD
\CORP
\end{gcl}
\caption{Basic implementation .}
\end{center}
\end{figure}

Once the recognition pass has filled the rules stream, the translation pass can take over. 
In much the same manner as any LR parser, the translation pass can simply iterate over the rules assembling the parse tree in a bottom up fashion. 
Because every grammar rule is a concrete instance of a grammar production, a parse tree can be constructed deterministically without reference to the lex stream 
other than to copy lexical tokens or their underlying data to the new parse tree structure.
Alternatively one could also construct this tree in a top-down fashion by simply iterating through the rules in reverse.
\begin{figure}[!ht]
\begin{center}
\begin{gcl}
\PROC TranslationPass(parseTree, rulesStream, lexStream, textStream)
treeNode \becomes parseTree;
Append(parseTree, rulesStream_{j});
j \becomes j + 1;
\CORP
\end{gcl}
\caption{An interpreter for the LD parsing DSL.}
\end{center}
\end{figure}

\subsection{Internal representation}
We will describe our parsers' internal representation as an encoded stream of opcodes accompanied by their arguments. 
We will refer to this stream as the \emph{bitcode}\footnote{This is perhaps more commonly refered to as \emph{bytecode}, refering to the idea that an opcode is encoded by a single byte. We've chosen to use the more general term \emph{bitcode} employed by the popular LLVM infrastructure.} encoding of a parser.
In our implementation the bitcodes for a single instruction is packed into a varying number of bytes.

An implementation may however choose any suitable internal representation and we anticipate that implementors may choose to translate the bitcode to the internal representation suitable for their target language or machine.
\texttt{opcode, [arguments...]}

We've made certain optimistic optimizations in order to minimize the cost of instruction decoding. 
For this reason an opcode is often a packed representation of the particular instruction.
In particular terminal tokens always represent $shift$ instructions while nonterminal tokens represent $reduce$ instructions.
This is done by using the highest bit to distinguish between terminals and nonterminals and hence also between $shift$ and $reduce$ actions.
An additional bit flags in highest byte is then used to modify the  $reduce$ instructions by changing it into the corresponding $resolve$ instruction.
Finally, certain opcode values are reserved for the remaining instructions which also include additional arguments.

\begin{itemize}
\item A special token called $ignore$ represents the $delay$ instruction and is also used as the placeholder token that is appended to the rules buffer by the $delay$ action.
\item The $switch$ opcode corresponds to the instruction by the same name. It is coded as the string\\\\
      \texttt{switch, length, (terminal, target\_state)\*}\\\\
      where $length$ is the number of pairs that map terminal tokens to their target states.
\item $return$ is the simple opcode corresponding to the same instruction but takes no arguments
\item $goto$ is the opcode corresponding to the instruction coded as\\\\
      \texttt{goto, source\_state, target\_state}\\
\item $accept$ is the opcode corresponding to the final $return$ instruction that ends the recognition pass.
\end{itemize}

This configuration makes decoding $shift$ instructions efficient and convenient since we can simply test for equality with lexical tokens read from the input stream. 
Next the opcode that is decoded to a $reduce$ or $resolve$ instruction if it matches, followed by tests for each of $switch$, $return$, $goto$ and finally $accept$.

\subsection{Parser construction algorithm}
Constructing a parser by hand is a time consuming and error prone process. For this reason an automatic construction algorithm is desireable.
As a prototype we have constructed a prototype parser generator that is sufficient for many grammars including grammars with lookahead greater than 1.\\
Given a grammar specification, the construction algorithm outputs the bitcode representation to be used with the interpreter.\\

We will use a state graph to represent various possible states that the parser can inhabit. 
Every \emph{state} is composed of a set of unique items. 
In turn an \emph{item} refers to specific grammar rule and a state cursor.
Finally states have two types of edges leading from one state to another. 
The first type represents $switch$ branches and the second represents $goto$ edges. We also distinguish between \emph{leading} and \emph{following} edges with reference to a particular state node.\\

Along with a grammar specification, a user must also specify the top-most nonterminal in the grammar which, given a conforming input stream, should form the root node of the final parse tree.\\
For example given the following grammar:

\parbox{.3\textwidth}{\begin{align*}
G \equiv \quad & A_1 \rightarrow x\\
               & B_1 \rightarrow x\\
               & B_2 \rightarrow x z\\
               & C_1 \rightarrow y\\
               & C_2 \rightarrow C y\\
               & S_1 \rightarrow A C a \$\\
               & S_2 \rightarrow B C b \$
\end{align*}}

The initial state will contain the rules corresponding the root production of the grammar, in this case $S_1$ and $S_2$.

\begin{figure}[!ht]
\begin{center}
\begin{tikzpicture}
   \path (0,0) node[draw,shape=rectangle] (S0) 
   {
     \parbox{0\textwidth}{\begin{align*}
     & S_1 \rightarrow . A C a \$\\
     & S_2 \rightarrow . B C b \$
     \end{align*}}
   };
   \path (-1.5,0.9) node (S0n) {0};
\end{tikzpicture}
\caption{An example root state node in the parser generator.}
\end{center}
\end{figure}

The period on the right hand side of each rule is a cursor indicating the progress of the rule in a particular parser state.
For example, given an item $W_1 \rightarrow x . y$, this would indicate that the terminal $x$ has been recognized, and only $y$ remains to be parsed in order to recognize this rule.\\

Once an initial state node has been constructed the main loop of the algorithm takes over. 
The state is expanded until a non-deterministic \emph{branch state} is reached where a parser would be required to make a choice corresponding to the $switch$ action.
During the expansion progresses, parsing instructions are recorded into an \emph{instruction table} which will be encoded into bitcode stream once the algorithm completes.
The instruction table is an array of dynamically growing `strings' of instructions. 
Each \emph{instruction string} corresponds to a particular branch state in the state graph.
An alternative view of this idea is that every instruction string could represent a procedure of a recursive ascent parser written in an imperative programming language.
The following steps iteratively expand the state until a branch state is reached.

\begin{enumerate}
\item Expand items --- The token directly to the right of every cursor is inspected. 
If the token is a nonterminal token, then the item is replaced with the following items: For every rule producing this nonterminal on the left, duplicate the item and annotate it with this rule number.
In addition a new item corresponding with the given rule and a cursor in the initial position may be added to the state whenever an old item is annotated. 
An exception is made only when such an item already exists in the state, in which case only the annotation takes place.
Once this process is completed the newly added (unannotated) items are again expanded until no more unnanoted items with nonterminals following the cursor can be found. 
For example, following the expansion procedure our example state will look like this.

\begin{figure}[!ht]
\begin{center}
\begin{tikzpicture}
   \path (0,0) node[draw,shape=rectangle] (S) 
   {
     \parbox{0\textwidth}{\begin{align*}
     & S_1 \rightarrow . A C a \$ \quad {(A_1)}\\
     & S_2 \rightarrow . B C b \$ \quad {(B_1)}\\
     & S_2 \rightarrow . B C b \$ \quad {(B_2)}\\
     & A_1 \rightarrow . x\\
     & B_1 \rightarrow . x\\
     & B_2 \rightarrow . x z
     \end{align*}}
   };
   \path (-2,2) node (S0n) {0};
\end{tikzpicture}
\caption{An example state after items have been expanded.}
\end{center}
\end{figure}

\item Shift terminal tokens --- Once all items have been expanded, each item is once again inspected. 
If a terminal token follows the cursor, it is moved one token forward and the terminal token is placed into a temporary set to be shifted 
known, for historical reasons, 
as the \emph{FIRST} set\footnote{LR(k) parsers make a distinction between FIRST set and FOLLOW sets indicating, respectively, 
the first terminal tokens that can be recognized in a string of grammar tokens and the sets of terminals that could follow each of the FIRST terminals.} of the current state.
Because there must be at least one terminal token in the grammar (specifically the end-of-stream token \$) there is at least one token in the FIRST set.
If only one unique terminal exists in the set, then the state is \emph{deterministic} and hence a $shift$ instruction will be generated and appended to the current string of instructions in the instruction table.
However, if multiple unique terminals exist in the FIRST set of the state, then the state is a non-deterministic branch state and a $switch$ instruction is generated instead.
\begin{itemize}
\item Given that a $shift$ instruction must be generated, \emph{TODO: ...}. Then the entire procedure is repeated until a set of FIRST terminals must once again be shifted.
\item Given that a $switch$ instruction must be generated, the current state must be duplicated once for every unique terminal in the switch. 
An edge labeled with the terminal token connects every new state to its source.
However, every duplicate state must be filtered so that only the rules partaking in a particular branch of the switch are included.
This is handled by checking all 
\end{itemize}
An item is \emph{completed} once the cursor has moved past all remaining tokens on the right hand side of the rule.
Once either a $shift$ or a $switch$ has been generated, several items in the state may be completed. 
A special subroutine is responsible for handling these completed items which in turn generates $reduce$ and $delay$ instructions depending on the context.
In the case of a $shift$ action, the items in the original state is completed whereas, in the case of a $switch$, each of the new states are processed by the routine.\\

\item[$\bullet$] Complete items --- Every time a FIRST set is processed, the generator must test whether all items in the state are complete. 
In the case of a $shift$ action, the current state is tested and if all items are complete, a $return$ instruction must be generated and all \emph{...(todo)}.
\end{enumerate}

For example, iterative application of steps 1 and 2 will result in the following state changes

\begin{figure}[!ht]
\begin{center}
\parbox{.3\textwidth}{\begin{tikzpicture}
   \path (0,0) node[draw,shape=rectangle] (S) 
   {
     \parbox{0\textwidth}{\begin{align*}
     & S_1 \rightarrow . A C a \$ \quad {(A_1)}\\
     & S_2 \rightarrow . B C b \$ \quad {(B_1)}\\
     & S_2 \rightarrow . B C b \$ \quad {(B_2)}\\
     & A_1 \rightarrow x .\\
     & B_1 \rightarrow x .\\
     & B_2 \rightarrow x . z
     \end{align*}}
   };
   \path (-2,2) node (S0n) {0};
\end{tikzpicture}}
\parbox{.65\textwidth}{Parsing instructions:\\$shift(x)$}
\caption{The $shift$ instruction is deduced by stepping over the $x$ terminal.}
\end{center}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\parbox{.3\textwidth}{\begin{tikzpicture}
   \path (0,0) node[draw,shape=rectangle] (S) 
   {
     \parbox{0\textwidth}{\begin{align*}
     & S_1 \rightarrow A . C a \$ \quad {(A_1)}\\
     & S_2 \rightarrow B . C b \$ \quad {(B_1)}\\
     & S_2 \rightarrow B . C b \$ \quad {(B_2)}\\
     & B_2 \rightarrow x . z
     \end{align*}}
   };
   \path (-2,1.5) node (S0n) {0};
\end{tikzpicture}}
\parbox{.65\textwidth}{Parsing instructions:\\$shift(x), delay(A_1, B_1)$}
\caption{A $delay$ instruction is deduced by completing rules $A_1$ and $B_1$. Take note of rule $B_2$ which cannot be completed due to the remaining terminal $z$.}
\end{center}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\parbox{.3\textwidth}{\begin{tikzpicture}
   \path (0,0) node[draw,shape=rectangle] (S)
   {
     \parbox{0\textwidth}{\begin{align*}
     & S_1 \rightarrow A . C a \$ \quad\\
     & S_2 \rightarrow B . C b \$ \quad\\
     & S_2 \rightarrow . B C b \$ \quad {(B_2)}\\
     & B_2 \rightarrow x . z\\	
     & C_1 \rightarrow . y\\
     & C_2 \rightarrow . C y
     \end{align*}}
   };
   \path (-2,2) node (S0n) {0};
\end{tikzpicture}}
\parbox{.65\textwidth}{Parsing instructions:\\$shift(x), delay(A_1, B_1), switch(y \mapsto 1, z \mapsto 2)$}
\caption{A $switch$ instruction is deduced by stepping over terminals $y$ and $z$.}
\end{center}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\parbox{.65\textwidth}{\begin{tikzpicture}
   \path (0,0) node[draw,shape=rectangle] (S0) 
   {
     \parbox{0\textwidth}{\begin{align*}
     & S_1 \rightarrow A . C a \$\\
     & S_2 \rightarrow B . C b \$\\
     & S_2 \rightarrow . B C b \$ \quad {(B_2)}\\
     & B_2 \rightarrow x . z\\	
     & C_1 \rightarrow . y\\
     & C_2 \rightarrow . C y
     \end{align*}}
   };
   \path (-2,2) node (S0n) {0};

   \path (5,2.5) node[draw,shape=rectangle] (S1) 
   {
     \parbox{0\textwidth}{\begin{align*}
     & S_1 \rightarrow A . C a \$\\
     & S_2 \rightarrow B . C b \$\\
     & C_1 \rightarrow . y\\
     & C_2 \rightarrow . C y
     \end{align*}}
   };
   \path (3.5,4) node (S1n) {1};

   \path (5.3,-2) node[draw,shape=rectangle] (S2) 
   {
     \parbox{0\textwidth}{\begin{align*}
     & S_2 \rightarrow . B C b \$ \quad {(B_2)}\\
     & B_2 \rightarrow x . z
     \end{align*}}
   };
   \path (3.3,-1.1) node (S2n) {2};

   %todo: edge lables and arrows
   \draw (S0) -- (S1) (S0) -- (S2);

\end{tikzpicture}}
\caption{Generating new states for the switch instruction.}
\end{center}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\begin{tikzpicture}
   \path (0,0) node[draw,shape=rectangle] (S1) 
   {
     \parbox{0\textwidth}{\begin{align*}
     & S_1 \rightarrow A . C a \$ \quad {(C_1)}\\
     & S_1 \rightarrow A . C a \$ \quad {(C_2)}\\
     & S_2 \rightarrow B . C b \$ \quad {(C_1)}\\
     & S_2 \rightarrow B . C b \$ \quad {(C_2)}\\
     & C_1 \rightarrow . y\\
     & C_2 \rightarrow . C y \quad {(C_1)}\\
     & C_2 \rightarrow . C y \quad {(C_2)}
     \end{align*}}
   };
   \path (-2,2.3) node (S1n) {1};
\end{tikzpicture}
\caption{Expanding state 1}
\end{center}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\parbox{.3\textwidth}{\begin{tikzpicture}
   \path (0,0) node[draw,shape=rectangle] (S2) 
   {
     \parbox{0\textwidth}{\begin{align*}
     & S_2 \rightarrow B . C b \$ \quad {(B_2)}\\
     & B_2 \rightarrow x z .
     \end{align*}}
   };
   \path (-2,0.9) node (S2n) {2};
\end{tikzpicture}}
\parbox{.65\textwidth}{Parsing instructions:\\$shift(z)$}
\caption{A $shift$ instruction is deduced by stepping over the $z$ terminal.}
\end{center}
\end{figure}

At this point our instruction string will contain a shift, delay and a switch instructions.
\begin{enumerate}
\item[0] $shift(x), delay, switch(y \mapsto 1, z \mapsto 2)$\\
\item $shift(y), switch(a \mapsto ?, b \mapsto ?, y \mapsto ?)$\\
\item $shift(z), shift(y), switch(b \mapsto ?, y \mapsto ?)$
\end{enumerate}

In the final section of this paper we will discuss the limitations of this algorithm and discuss some proposals for extension that take into account the full set of context free grammars. 

(TODO: We will also show that parsers built using this technique already includes the entire class of LR(1) grammars)

\section*{Results}
...
While it is tempting to classify an LD parser according to the number of operations per input token, we should also take into account the class of grammar being translated.

For simplicity only the recognition pass is considered here as the translation pass corresponds closely to that of any lr(0) parser.

Analyzing the performance of each instruction individually is relatively easy. 
For this reason it is easiest to characterize the performance of LD parsers according to the number of parsing instructions an LD parser may take.
The table bellow lists the performance of each instruction according to its algorithmic worst-case complexity.

$shift$: constant
$switch$: linear in the number of arguments (todo)
$accept$/$return$: constant (todo)
$goto$: linear in the number of arguments (todo)
$delay$: constant
$reduce$: constant
$resolve$: constant

Given an input stream of $n$ tokens, every token in the input stream may only be tested once during the recognition pass.
Hence, the number of $shift$ and $switch$ instructions executed by the parse will certainly be linear in $n$.

\emph{HOLD ON: THE STUFF BELOW ABOUT LR(k) having a decideable depth seems unlikely!!!!!!!!!!!!! LR(k) may still include recursive rules)}

The number of $reduce$ instructions is intuitively related to both the number input tokens and the complexity of the given grammar specification.
However, since $reduce$ instructions do not take any lookahead into account we believe that $reduce$ instructions will have the same worst-case asymptotic
performance regardless of the class of grammar parsed. (
The performance can be related to $n$, the number of input tokens and $j$, the maximum depth of the final parse tree.
Let us first consider cases in which the grammar falls withing the class $LR(k)$ for some fixed $k$.
Then in the worst case the number of reduce instructions will be $(j - 1) \mul n + 1$ since every token will be involved in at most $j$ productions, one of which has to be the root node of the parse tree.
For any such grammar that is known to have a bounded $k$, computation for a maximal value of $j$ is decidable and hence most analyses of $LR(k)$ algorithms simply include $j$ in the constant term 
of the grammar to approximate the performance of the algorithm as $O(n)$.
However, if the the grammar is unambiguous, but $k$ is unbounded, but $j$ is not so easily determined. 
What we can prove is that $j$ will be bounded above by the product of $n$ and some constant.
Hence reductions will be in the order $O(n^2)$. 

From a practical point of view we expect that sane grammars will not include many redundant \emph{alternative wording?} productions, especially inside recursive rules.
Based on this assumption, it is our estimation that a typical average case performance of $o(n)$ is far more likely for most applications. (An argument that could most likely be applied to GLR and Earley parsers as well).
%Let us suppose for \emph{posterity?} sake that a grammar specification will not include redundant \emph{alternative wording?} productions. 
%In other words every grammar rule has at least two tokens on the right-hand side. Then it is clear that $n log(n)$ would bound the number of $reduce$ instructions.
%%%%%%%%%%

Following the same argumentation, similar upper bounds can be found for each of .... \emph{todo}
Pairs of $delay$ and $resolve$ or $delay$ and $ignore$ instructions behave somewhat similarly to the $reduce$ instruction. 
Note that $delay$ actions have been designed to be cheap in both the recognition and translation passes. 
%Hence, in relation to GLR they should take only a minimal number of cycles?

$goto$'s on the other hand is dependent on the lookahead size.


It is easy to see that an LD parser will never backtrack.


\section*{Conclusions}

Delayed actions show promise for .

\section*{Future work}

There remains a large amount of theoretical and practical work to be done.
Our main concern is increasing the generality of LD parsing in order to handle a larger class of context-free grammars, possibly including all ambiguous grammars. 
We note that it is possible to annotate delay actions with the possible reductions that could be performed. 
Generating new rule number for various combinations of these rules may allow us to generate parse trees with ambiguities.

It has been established that LD parsing instruction set is capable of handling a wide variety of unambiguous context-free grammars but will not accept the special case where lookahead is required within nested recursion.
It is put forward that the instruction set could be extended, possibly by adding an alternative to the $return$ instruction.

Knuth showed that determining whether a context-free grammar is ambiguous is undecidable in general\cite{knuth65}. 
This implies that certain concequences must be taken into account for our work. First, since we construct parsers .
Second, we suspect that this problem may lead to infinite growth in the state graph during the construction algorithm if certain ambiguous grammars are used.
In order to prevent this effect, we may simply specify a limiting depth for the generator when generating $switch$ actions.
Even so, the problem warrants further study.

We have given a rough analysis of the performance of an LD parser. 
Benchmarks must be set up in order to test the performance of generated parsers in comparison to other parser generators.
While algorithmic analyses provides a usefull metric to test the scalability of parsing algorithms, the constant term tends to have a large effect on the performance in practice and 
complex effects on performance such as memory latency and branch prediction may be missed by over-simplified mathematical models.
The simplicity of our approach emboldens us to explore efficiency of generated parsers.
The presented construction algorithm provides many opportunities for further optimizations in the generated bitcode.
Another possibility that could provide an additional speed-up is direct conversion to machine code using existing optimizing compilers that already employ sophisticated static analyses.

Finally, there are additional engineering concerns that also need to be resolved before LD parsing can be considered for industrial use.
In particular we have omitted error recovery from our work thus far.

\emph{ %BUSY HERE
(TODO)\\
Open problems: problematic unambiguous context-free grammars (nested lookahead problem + construction problem), progressive parsing (combining the recognition and builder passes), formal proofs of correctness\\
Suggest that delays should be encoded as stacks of continuations similar to functional programming rather than as a simple callstack?\\
Performance analysis and benchmarks\\
Termination analysis\\
Proof is needed for the absence of exponential growth of states in the generated parser\\
Parallelization / pipelined implementations?\\
Application to other grammar classes (E.g. ambiguous context-free and context-sensitive grammars)\\
Perhaps applications to other dynamic programming problems? How does it relate to automatic memoization in functional programming? (I.e. Memoization is an automatically deduced top-down dynamic programming mechanism. Is it possible to find bottom-up/hybrid solutions automatically?)
}

\emph{TODO: Check...} Finally, much of the research into parsing algorithms occured in the 70's and 80's. Today's hardware has different performance characteristics which is still in a phase of rapid change. 
In order to take advantage of modern hardware, now and in the future, we may need to re-evaluate the classical algorithms using different performance metrics.

\section*{Acknowledgements}
I would like to thank Prof. Derrick Kourie and Prof. Bruce Watson of the University of Pretoria for their insightful suggestions and patient support during the development of this work.

\bibliographystyle{plain}
\bibliography{delayedcase}

\end{document}

